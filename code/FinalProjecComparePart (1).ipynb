{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6d3225c0-2189-4a08-98ff-e82252d1b4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder, Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "import pyswarms as ps\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "396a36b3-7df9-4511-977d-60a63f415da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_score(y_true, y_pred):\n",
    "    contingency_matrix = np.zeros((len(np.unique(y_true)), len(np.unique(y_pred))))\n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        contingency_matrix[true_label, pred_label] += 1\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / len(y_true)\n",
    "\n",
    "def evaluate_clustering(y_true, y_pred):\n",
    "    return {\n",
    "        'Purity': purity_score(y_true, y_pred),\n",
    "        'NMI': normalized_mutual_info_score(y_true, y_pred),\n",
    "        'ARI': adjusted_rand_score(y_true, y_pred)\n",
    "    }\n",
    "    \n",
    "def preprocess_text(texts):\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    processed_texts = []\n",
    "\n",
    "    for text in texts:\n",
    "        text = re.sub(r'\\b\\d+\\b', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'0x[0-9a-fA-F]+', '', text)\n",
    "        text = re.sub(r'[_]+', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.lower()\n",
    "        tokens = word_tokenize(text)\n",
    "        filtered_tokens = [\n",
    "            lemmatizer.lemmatize(token)\n",
    "            for token in tokens\n",
    "            if token.isalpha() and token not in stop_words and len(token) >= 3\n",
    "        ]\n",
    "        processed_texts.append(' '.join(filtered_tokens))\n",
    "\n",
    "    return processed_texts\n",
    "\n",
    "\n",
    "\n",
    "class SphericalKMeans:\n",
    "    def __init__(self, n_clusters, max_iter=100):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        \n",
    "        normalizer = Normalizer(norm='l2')\n",
    "        X_norm = normalizer.fit_transform(X.toarray() if hasattr(X, 'toarray') else X)\n",
    "        \n",
    "        \n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, init='k-means++')\n",
    "        centers = kmeans.fit(X_norm).cluster_centers_\n",
    "        \n",
    "        for _ in range(self.max_iter):\n",
    "            \n",
    "            similarities = cosine_similarity(X_norm, centers)\n",
    "            labels = np.argmax(similarities, axis=1)\n",
    "            \n",
    "            \n",
    "            new_centers = np.array([X_norm[labels == i].mean(axis=0) \n",
    "                                  for i in range(self.n_clusters)])\n",
    "            \n",
    "            \n",
    "            for i in range(self.n_clusters):\n",
    "                if np.isnan(new_centers[i]).any():\n",
    "                    new_centers[i] = centers[i]\n",
    "            \n",
    "            \n",
    "            new_centers = normalizer.transform(new_centers)\n",
    "            \n",
    "            if np.allclose(centers, new_centers):\n",
    "                break\n",
    "            centers = new_centers\n",
    "        \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0a2ea0e-87f9-4340-8e08-95bf0df598f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=10.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computer_KM:\n",
      "Purity: 0.3318\n",
      "NMI: 0.1359\n",
      "ARI: 0.0392\n",
      "\n",
      "Computer_SKM:\n",
      "Purity: 0.3950\n",
      "NMI: 0.1655\n",
      "ARI: 0.0983\n",
      "\n",
      "Computer_LSAKM:\n",
      "Purity: 0.3881\n",
      "NMI: 0.1575\n",
      "ARI: 0.0963\n",
      "\n",
      "Politics_KM:\n",
      "Purity: 0.4640\n",
      "NMI: 0.1205\n",
      "ARI: 0.0435\n",
      "\n",
      "Politics_SKM:\n",
      "Purity: 0.6290\n",
      "NMI: 0.3314\n",
      "ARI: 0.2838\n",
      "\n",
      "Politics_LSAKM:\n",
      "Purity: 0.5878\n",
      "NMI: 0.2881\n",
      "ARI: 0.1865\n",
      "\n",
      "Miscellaneous_KM:\n",
      "Purity: 0.5894\n",
      "NMI: 0.4066\n",
      "ARI: 0.2277\n",
      "\n",
      "Miscellaneous_SKM:\n",
      "Purity: 0.7006\n",
      "NMI: 0.5224\n",
      "ARI: 0.4341\n",
      "\n",
      "Miscellaneous_LSAKM:\n",
      "Purity: 0.7101\n",
      "NMI: 0.4987\n",
      "ARI: 0.4704\n",
      "\n",
      "Religion_KM:\n",
      "Purity: 0.4493\n",
      "NMI: 0.0322\n",
      "ARI: 0.0172\n",
      "\n",
      "Religion_SKM:\n",
      "Purity: 0.4806\n",
      "NMI: 0.0630\n",
      "ARI: 0.0500\n",
      "\n",
      "Religion_LSAKM:\n",
      "Purity: 0.5507\n",
      "NMI: 0.0826\n",
      "ARI: 0.0925\n",
      "\n",
      "Science_KM:\n",
      "Purity: 0.4828\n",
      "NMI: 0.3122\n",
      "ARI: 0.1032\n",
      "\n",
      "Science_SKM:\n",
      "Purity: 0.7082\n",
      "NMI: 0.4873\n",
      "ARI: 0.3343\n",
      "\n",
      "Science_LSAKM:\n",
      "Purity: 0.7098\n",
      "NMI: 0.3932\n",
      "ARI: 0.3689\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class SphericalKMeans:\n",
    "    def __init__(self, n_clusters, max_iter=100):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        \n",
    "        normalizer = Normalizer(norm='l2')\n",
    "        X_norm = normalizer.fit_transform(X.toarray() if hasattr(X, 'toarray') else X)\n",
    "        \n",
    "        \n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, init='k-means++')\n",
    "        centers = kmeans.fit(X_norm).cluster_centers_\n",
    "        \n",
    "        for _ in range(self.max_iter):\n",
    "            \n",
    "            similarities = cosine_similarity(X_norm, centers)\n",
    "            labels = np.argmax(similarities, axis=1)\n",
    "            \n",
    "            \n",
    "            new_centers = np.array([X_norm[labels == i].mean(axis=0) \n",
    "                                  for i in range(self.n_clusters)])\n",
    "            \n",
    "            \n",
    "            for i in range(self.n_clusters):\n",
    "                if np.isnan(new_centers[i]).any():\n",
    "                    new_centers[i] = centers[i]\n",
    "            \n",
    "            \n",
    "            new_centers = normalizer.transform(new_centers)\n",
    "            \n",
    "            if np.allclose(centers, new_centers):\n",
    "                break\n",
    "            centers = new_centers\n",
    "        \n",
    "        return labels\n",
    "        \n",
    "\n",
    "\n",
    "def run_experiments():\n",
    "    categories = {\n",
    "        'Computer': ['comp.graphics', 'comp.os.ms-windows.misc', \n",
    "                    'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', \n",
    "                    'comp.windows.x'],\n",
    "        'Politics': ['talk.politics.misc', 'talk.politics.guns', \n",
    "                    'talk.politics.mideast'],\n",
    "        'Miscellaneous': ['misc.forsale','talk.politics.misc','talk.religion.misc','comp.os.ms-windows.misc'],\n",
    "        'Religion': ['talk.religion.misc', 'alt.atheism', 'soc.religion.christian'],\n",
    "        'Science': ['sci.crypt', 'sci.electronics', 'sci.med', 'sci.space']\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, subcats in categories.items():\n",
    "        \n",
    "        dataset = fetch_20newsgroups(subset='all', categories=subcats, \n",
    "                                   remove=('headers', 'footers', 'quotes'))\n",
    "        n_clusters = len(subcats)\n",
    "        \n",
    "        processed_texts = preprocess_text(dataset.data)\n",
    "\n",
    "\n",
    "        vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                     min_df=2,\n",
    "                     max_df=0.95,\n",
    "                     token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "        \n",
    "        X = vectorizer.fit_transform(processed_texts)\n",
    "        y_true = LabelEncoder().fit_transform(dataset.target)\n",
    "        \n",
    "        # 1. K-Means\n",
    "        km = KMeans(n_clusters=n_clusters, init='k-means++')\n",
    "        y_pred = km.fit_predict(X)\n",
    "        results[f\"{name}_KM\"] = evaluate_clustering(y_true, y_pred)\n",
    "        \n",
    "        # 2. Spherical K-Means (SKM)\n",
    "        skm = SphericalKMeans(n_clusters=n_clusters)\n",
    "        y_pred = skm.fit_predict(X)\n",
    "        results[f\"{name}_SKM\"] = evaluate_clustering(y_true, y_pred)\n",
    "        \n",
    "        # 3. LSAKM\n",
    "        svd = TruncatedSVD(n_components=5)\n",
    "        normalizer = Normalizer(copy=False)\n",
    "        lsa = make_pipeline(svd, normalizer)\n",
    "        X_lsa = lsa.fit_transform(X)\n",
    "        km_lsa = KMeans(n_clusters=n_clusters, init='k-means++')\n",
    "        y_pred = km_lsa.fit_predict(X_lsa)\n",
    "        results[f\"{name}_LSAKM\"] = evaluate_clustering(y_true, y_pred)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "final_results = run_experiments()\n",
    "\n",
    "\n",
    "for name, metrics in final_results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84417f33-5f8c-467b-abe7-b756875bf3df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=10.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=10.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=10.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=10.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=10.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Final Aggregated Results:\n",
      "\n",
      " Computer_KM\n",
      "Purity: Mean = 0.2672, Std = 0.0640\n",
      "NMI: Mean = 0.0679, Std = 0.0609\n",
      "ARI: Mean = 0.0177, Std = 0.0190\n",
      "\n",
      " Computer_SKM\n",
      "Purity: Mean = 0.4144, Std = 0.0156\n",
      "NMI: Mean = 0.1733, Std = 0.0117\n",
      "ARI: Mean = 0.1100, Std = 0.0104\n",
      "\n",
      " Computer_LSAKM\n",
      "Purity: Mean = 0.4029, Std = 0.0228\n",
      "NMI: Mean = 0.1722, Std = 0.0160\n",
      "ARI: Mean = 0.1119, Std = 0.0258\n",
      "\n",
      " Politics_KM\n",
      "Purity: Mean = 0.4670, Std = 0.0642\n",
      "NMI: Mean = 0.1480, Std = 0.0925\n",
      "ARI: Mean = 0.0591, Std = 0.0401\n",
      "\n",
      " Politics_SKM\n",
      "Purity: Mean = 0.5903, Std = 0.0463\n",
      "NMI: Mean = 0.2997, Std = 0.0680\n",
      "ARI: Mean = 0.2179, Std = 0.0717\n",
      "\n",
      " Politics_LSAKM\n",
      "Purity: Mean = 0.5904, Std = 0.0305\n",
      "NMI: Mean = 0.2582, Std = 0.0300\n",
      "ARI: Mean = 0.2033, Std = 0.0402\n",
      "\n",
      " Miscellaneous_KM\n",
      "Purity: Mean = 0.6278, Std = 0.0276\n",
      "NMI: Mean = 0.4312, Std = 0.0219\n",
      "ARI: Mean = 0.2893, Std = 0.0183\n",
      "\n",
      " Miscellaneous_SKM\n",
      "Purity: Mean = 0.6952, Std = 0.0409\n",
      "NMI: Mean = 0.4835, Std = 0.0213\n",
      "ARI: Mean = 0.4390, Std = 0.0309\n",
      "\n",
      " Miscellaneous_LSAKM\n",
      "Purity: Mean = 0.6796, Std = 0.0052\n",
      "NMI: Mean = 0.4785, Std = 0.0057\n",
      "ARI: Mean = 0.4306, Std = 0.0086\n",
      "\n",
      " Religion_KM\n",
      "Purity: Mean = 0.4927, Std = 0.0484\n",
      "NMI: Mean = 0.0639, Std = 0.0304\n",
      "ARI: Mean = 0.0435, Std = 0.0290\n",
      "\n",
      " Religion_SKM\n",
      "Purity: Mean = 0.5013, Std = 0.0387\n",
      "NMI: Mean = 0.0686, Std = 0.0275\n",
      "ARI: Mean = 0.0613, Std = 0.0251\n",
      "\n",
      " Religion_LSAKM\n",
      "Purity: Mean = 0.5535, Std = 0.0059\n",
      "NMI: Mean = 0.0863, Std = 0.0069\n",
      "ARI: Mean = 0.0950, Std = 0.0062\n",
      "\n",
      " Science_KM\n",
      "Purity: Mean = 0.4519, Std = 0.1048\n",
      "NMI: Mean = 0.2240, Std = 0.1251\n",
      "ARI: Mean = 0.0927, Std = 0.0472\n",
      "\n",
      " Science_SKM\n",
      "Purity: Mean = 0.6382, Std = 0.0735\n",
      "NMI: Mean = 0.3949, Std = 0.0651\n",
      "ARI: Mean = 0.3101, Std = 0.0785\n",
      "\n",
      " Science_LSAKM\n",
      "Purity: Mean = 0.7089, Std = 0.0097\n",
      "NMI: Mean = 0.3677, Std = 0.0237\n",
      "ARI: Mean = 0.3697, Std = 0.0135\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "n_runs = 5\n",
    "aggregated_results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n Running iteration {run + 1}...\")\n",
    "    run_results = run_experiments()\n",
    "    for method_name, metrics in run_results.items():\n",
    "        for metric_name, value in metrics.items():\n",
    "            aggregated_results[method_name][metric_name].append(value)\n",
    "\n",
    "print(\"\\n Final Aggregated Results:\")\n",
    "for method_name, metric_values in aggregated_results.items():\n",
    "    print(f\"\\n {method_name}\")\n",
    "    for metric_name, values in metric_values.items():\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        print(f\"{metric_name}: Mean = {mean_val:.4f}, Std = {std_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53bd9d54-3dc6-43ca-b4b5-9a18a4679f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=9.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KM:\n",
      "Purity: 0.8980\n",
      "NMI: 0.7614\n",
      "ARI: 0.7606\n",
      "\n",
      "SKM:\n",
      "Purity: 0.7991\n",
      "NMI: 0.6957\n",
      "ARI: 0.6514\n",
      "\n",
      "LSAKM:\n",
      "Purity: 0.9330\n",
      "NMI: 0.8131\n",
      "ARI: 0.8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=9.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#bbc news\n",
    "def run_experiments_custom_dataset():\n",
    "    \n",
    "    df = pd.read_csv(\"C:/Users/asus/Downloads/FinalProject/dataset.csv\", encoding='latin1')\n",
    "\n",
    "    texts = df['news'].astype(str).tolist()\n",
    "    labels = df['type'].astype(str).tolist()\n",
    "    y_true = LabelEncoder().fit_transform(labels)\n",
    "    n_clusters = len(np.unique(y_true))\n",
    "\n",
    "    processed_texts = preprocess_text(texts)\n",
    "\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                 min_df=2,\n",
    "                 max_df=0.95,\n",
    "                 token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "   \n",
    "    \n",
    "    X = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    \n",
    "    km = KMeans(n_clusters=n_clusters, init='k-means++')\n",
    "    y_pred = km.fit_predict(X)\n",
    "    results[\"KM\"] = evaluate_clustering(y_true, y_pred)\n",
    "\n",
    "    \n",
    "    skm = SphericalKMeans(n_clusters=n_clusters)\n",
    "    y_pred = skm.fit_predict(X)\n",
    "    results[\"SKM\"] = evaluate_clustering(y_true, y_pred)\n",
    "\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=min(5, X.shape[1]))\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "    X_lsa = lsa.fit_transform(X)\n",
    "    km_lsa = KMeans(n_clusters=n_clusters, init='k-means++')\n",
    "    y_pred = km_lsa.fit_predict(X_lsa)\n",
    "    results[\"LSAKM\"] = evaluate_clustering(y_true, y_pred)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "results = run_experiments_custom_dataset()\n",
    "\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dadfd814-53e4-4527-8aa2-bfcc94b5be9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=9.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=9.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=9.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=9.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=9.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=9.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=9.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=9.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=9.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Final Aggregated Results:\n",
      "\n",
      " KM\n",
      "Purity: Mean = 0.8004, Std = 0.1135\n",
      "NMI: Mean = 0.6793, Std = 0.1097\n",
      "ARI: Mean = 0.5986, Std = 0.1919\n",
      "\n",
      " SKM\n",
      "Purity: Mean = 0.8604, Std = 0.0944\n",
      "NMI: Mean = 0.7431, Std = 0.0934\n",
      "ARI: Mean = 0.7361, Std = 0.1416\n",
      "\n",
      " LSAKM\n",
      "Purity: Mean = 0.9325, Std = 0.0004\n",
      "NMI: Mean = 0.8120, Std = 0.0009\n",
      "ARI: Mean = 0.8457, Std = 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=9.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "n_runs = 5\n",
    "aggregated_results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n Running iteration {run + 1}...\")\n",
    "    run_results = run_experiments_custom_dataset()\n",
    "    for method_name, metrics in run_results.items():\n",
    "        for metric_name, value in metrics.items():\n",
    "            aggregated_results[method_name][metric_name].append(value)\n",
    "\n",
    "print(\"\\n Final Aggregated Results:\")\n",
    "for method_name, metric_values in aggregated_results.items():\n",
    "    print(f\"\\n {method_name}\")\n",
    "    for metric_name, values in metric_values.items():\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        print(f\"{metric_name}: Mean = {mean_val:.4f}, Std = {std_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9d09c1e6-6e53-4048-8f62-f21489a5d038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KM:\n",
      "Purity: 0.8659\n",
      "NMI: 0.0205\n",
      "ARI: -0.0510\n",
      "\n",
      "SKM:\n",
      "Purity: 0.8819\n",
      "NMI: 0.3562\n",
      "ARI: 0.4994\n",
      "\n",
      "LSAKM:\n",
      "Purity: 0.8659\n",
      "NMI: 0.0818\n",
      "ARI: 0.0185\n"
     ]
    }
   ],
   "source": [
    "#SMS\n",
    "def run_experiments_SMS_dataset():\n",
    "    \n",
    "    df = pd.read_csv(\"C:/Users/asus/Downloads/FinalProject/archive (3)/spam.csv\", encoding='latin1')[['v1', 'v2']]\n",
    "    df.columns = ['label', 'text']\n",
    "\n",
    "    processed_texts = preprocess_text(df['text'].tolist())\n",
    "\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                 min_df=2,\n",
    "                 max_df=0.95,\n",
    "                 token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "\n",
    "    X = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "    label_mapping = {'ham': 0, 'spam': 1}\n",
    "    df['label'] = df['label'].map(label_mapping)\n",
    "\n",
    "    y_true = df['label'].tolist()\n",
    "\n",
    "    n_clusters = len(np.unique(y_true))\n",
    "    results = {}\n",
    "    km = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42)\n",
    "    y_pred = km.fit_predict(X)\n",
    "    results[\"KM\"] = evaluate_clustering(y_true, y_pred)\n",
    "\n",
    "    \n",
    "    skm = SphericalKMeans(n_clusters=n_clusters)\n",
    "    y_pred = skm.fit_predict(X)\n",
    "    results[\"SKM\"] = evaluate_clustering(y_true, y_pred)\n",
    "\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=min(10, X.shape[1]))\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "    X_lsa = lsa.fit_transform(X)\n",
    "    km_lsa = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42)\n",
    "    y_pred = km_lsa.fit_predict(X_lsa)\n",
    "    results[\"LSAKM\"] = evaluate_clustering(y_true, y_pred)\n",
    "    return results\n",
    "\n",
    "\n",
    "results = run_experiments_SMS_dataset()\n",
    "\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02ffb7c0-b0da-4203-9875-763c0bcefcb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 1...\n",
      "\n",
      " Running iteration 2...\n",
      "\n",
      " Running iteration 3...\n",
      "\n",
      " Running iteration 4...\n",
      "\n",
      " Running iteration 5...\n",
      "\n",
      " Final Aggregated Results:\n",
      "\n",
      " KM\n",
      "Purity: Mean = 0.8705, Std = 0.0090\n",
      "NMI: Mean = 0.0400, Std = 0.0649\n",
      "ARI: Mean = 0.0733, Std = 0.1202\n",
      "\n",
      " SKM\n",
      "Purity: Mean = 0.8683, Std = 0.0048\n",
      "NMI: Mean = 0.1615, Std = 0.1410\n",
      "ARI: Mean = 0.1800, Std = 0.2344\n",
      "\n",
      " LSAKM\n",
      "Purity: Mean = 0.8725, Std = 0.0131\n",
      "NMI: Mean = 0.1562, Std = 0.1299\n",
      "ARI: Mean = 0.1427, Std = 0.2218\n"
     ]
    }
   ],
   "source": [
    "n_runs = 5\n",
    "aggregated_results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n Running iteration {run + 1}...\")\n",
    "    run_results = run_experiments_SMS_dataset()\n",
    "    for method_name, metrics in run_results.items():\n",
    "        for metric_name, value in metrics.items():\n",
    "            aggregated_results[method_name][metric_name].append(value)\n",
    "\n",
    "print(\"\\n Final Aggregated Results:\")\n",
    "for method_name, metric_values in aggregated_results.items():\n",
    "    print(f\"\\n {method_name}\")\n",
    "    for metric_name, values in metric_values.items():\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        print(f\"{metric_name}: Mean = {mean_val:.4f}, Std = {std_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1a4600a6-3001-4a90-8768-57e1e137b27f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_6200\\619845970.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=min(N, len(x)), random_state=42))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KM:\n",
      "Purity: 0.2297\n",
      "NMI: 0.1517\n",
      "ARI: 0.0309\n",
      "\n",
      "SKM:\n",
      "Purity: 0.2951\n",
      "NMI: 0.1803\n",
      "ARI: 0.0766\n",
      "\n",
      "LSAKM:\n",
      "Purity: 0.3169\n",
      "NMI: 0.1880\n",
      "ARI: 0.0976\n"
     ]
    }
   ],
   "source": [
    "#DMOZ dataset\n",
    "def run_experiments_DMOZ_dataset():\n",
    "    df = pd.read_csv(\"C:/Users/asus/Downloads/FinalProject/archive (5)/dmoz.csv\", encoding='latin1')\n",
    "    df = df.rename(columns={\n",
    "                'category': 'Class Index',\n",
    "                'title': 'Title',\n",
    "                'desc': 'Description'\n",
    "            })\n",
    "    N = 300  \n",
    "    df_balanced = (\n",
    "        df.groupby('Class Index', group_keys=False)\n",
    "        .apply(lambda x: x.sample(n=min(N, len(x)), random_state=42))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "    texts = (df_balanced['Title'] + ' ' + df_balanced['Description']).astype(str).tolist()\n",
    "    labels = df_balanced['Class Index'].tolist()\n",
    "\n",
    "\n",
    "    processed_texts = preprocess_text(texts)\n",
    "\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                 min_df=2,\n",
    "                 max_df=0.95,\n",
    "                 token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "\n",
    "    X = vectorizer.fit_transform(processed_texts)\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_true = label_encoder.fit_transform(labels)\n",
    "\n",
    "    n_clusters = len(np.unique(y_true))\n",
    "    results = {}\n",
    "    km = KMeans(n_clusters=n_clusters, init='k-means++')\n",
    "    y_pred = km.fit_predict(X)\n",
    "    results[\"KM\"] = evaluate_clustering(y_true, y_pred)\n",
    "\n",
    "    \n",
    "    skm = SphericalKMeans(n_clusters=n_clusters)\n",
    "    y_pred = skm.fit_predict(X)\n",
    "    results[\"SKM\"] = evaluate_clustering(y_true, y_pred)\n",
    "\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=min(15, X.shape[1]))\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "    X_lsa = lsa.fit_transform(X)\n",
    "    km_lsa = KMeans(n_clusters=n_clusters, init='k-means++')\n",
    "    y_pred = km_lsa.fit_predict(X_lsa)\n",
    "    results[\"LSAKM\"] = evaluate_clustering(y_true, y_pred)\n",
    "    return results\n",
    "    \n",
    "results = run_experiments_DMOZ_dataset()\n",
    "\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d11f12a8-773f-467c-9a8a-5e017554fed4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_6200\\619845970.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=min(N, len(x)), random_state=42))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_6200\\619845970.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=min(N, len(x)), random_state=42))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_6200\\619845970.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=min(N, len(x)), random_state=42))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_6200\\619845970.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=min(N, len(x)), random_state=42))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_6200\\619845970.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=min(N, len(x)), random_state=42))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Final Aggregated Results:\n",
      "\n",
      " KM\n",
      "Purity: Mean = 0.2451, Std = 0.0102\n",
      "NMI: Mean = 0.1670, Std = 0.0088\n",
      "ARI: Mean = 0.0317, Std = 0.0038\n",
      "\n",
      " SKM\n",
      "Purity: Mean = 0.3039, Std = 0.0035\n",
      "NMI: Mean = 0.1955, Std = 0.0082\n",
      "ARI: Mean = 0.0846, Std = 0.0040\n",
      "\n",
      " LSAKM\n",
      "Purity: Mean = 0.3277, Std = 0.0053\n",
      "NMI: Mean = 0.2019, Std = 0.0067\n",
      "ARI: Mean = 0.1110, Std = 0.0041\n"
     ]
    }
   ],
   "source": [
    "n_runs = 5\n",
    "aggregated_results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n Running iteration {run + 1}...\")\n",
    "    run_results = run_experiments_DMOZ_dataset()\n",
    "    for method_name, metrics in run_results.items():\n",
    "        for metric_name, value in metrics.items():\n",
    "            aggregated_results[method_name][metric_name].append(value)\n",
    "\n",
    "print(\"\\n Final Aggregated Results:\")\n",
    "for method_name, metric_values in aggregated_results.items():\n",
    "    print(f\"\\n {method_name}\")\n",
    "    for metric_name, values in metric_values.items():\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        print(f\"{metric_name}: Mean = {mean_val:.4f}, Std = {std_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00c128f3-0780-4d9f-aa7f-296d09ec9b5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results on BBCSport Dataset ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KMeans:\n",
      "Purity: 0.3813\n",
      "NMI: 0.0582\n",
      "ARI: 0.0275\n",
      "\n",
      "SphericalKMeans:\n",
      "Purity: 0.8046\n",
      "NMI: 0.6996\n",
      "ARI: 0.5755\n",
      "\n",
      "LSA+KMeans:\n",
      "Purity: 0.7938\n",
      "NMI: 0.6723\n",
      "ARI: 0.5817\n"
     ]
    }
   ],
   "source": [
    "#BBC Sport\n",
    "def run_clustering(name, X, y, n_clusters):\n",
    "    print(f\"\\n=== Results on {name} Dataset ===\")\n",
    "    results = {}\n",
    "\n",
    "    # KMeans\n",
    "    km = KMeans(n_clusters=n_clusters, init='k-means++', n_init=10)\n",
    "    y_pred_km = km.fit_predict(X)\n",
    "    results['KMeans'] = evaluate_clustering(y, y_pred_km)\n",
    "\n",
    "    # SphericalKMeans\n",
    "    skm = SphericalKMeans(n_clusters=n_clusters)\n",
    "    y_pred_skm = skm.fit_predict(X)\n",
    "    results['SphericalKMeans'] = evaluate_clustering(y, y_pred_skm)\n",
    "\n",
    "    # LSA + KMeans\n",
    "    svd = TruncatedSVD(n_components=7)\n",
    "    lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "    X_lsa = lsa.fit_transform(X)\n",
    "\n",
    "    km_lsa = KMeans(n_clusters=n_clusters, init='k-means++',  n_init=10)\n",
    "    y_pred_lsa_km = km_lsa.fit_predict(X_lsa)\n",
    "    results['LSA+KMeans'] = evaluate_clustering(y, y_pred_lsa_km)\n",
    "\n",
    "    for method, metrics in results.items():\n",
    "        print(f\"\\n{method}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "\n",
    "bbcsport_X = pd.read_csv(r'C:\\Users\\asus\\Downloads\\FinalProject\\archive (2)\\bbcsport_mtx.csv', header=None).values\n",
    "bbcsport_y = pd.read_csv(r'C:\\Users\\asus\\Downloads\\FinalProject\\archive (2)\\bbcsport_classes.csv', header=None)[0].values\n",
    "bbcsport_n_clusters = len(np.unique(bbcsport_y))\n",
    "\n",
    "run_clustering(\"BBCSport\", bbcsport_X, bbcsport_y, bbcsport_n_clusters)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f9054176-8642-4360-9c65-3b3e8c5378a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Final Aggregated Results:\n",
      "\n",
      " KMeans\n",
      "Purity: Mean = 0.4814, Std = 0.0582\n",
      "NMI: Mean = 0.2235, Std = 0.1057\n",
      "ARI: Mean = 0.0913, Std = 0.0764\n",
      "\n",
      " SphericalKMeans\n",
      "Purity: Mean = 0.8152, Std = 0.0756\n",
      "NMI: Mean = 0.6999, Std = 0.0893\n",
      "ARI: Mean = 0.6538, Std = 0.1111\n",
      "\n",
      " LSA+KMeans\n",
      "Purity: Mean = 0.7938, Std = 0.0000\n",
      "NMI: Mean = 0.6730, Std = 0.0000\n",
      "ARI: Mean = 0.5826, Std = 0.0000\n"
     ]
    }
   ],
   "source": [
    "def run_clustering(name, X, y, n_clusters):\n",
    "    results = {}\n",
    "\n",
    "    # KMeans\n",
    "    km = KMeans(n_clusters=n_clusters, init='k-means++', n_init=10)\n",
    "    y_pred_km = km.fit_predict(X)\n",
    "    results['KMeans'] = evaluate_clustering(y, y_pred_km)\n",
    "\n",
    "    # SphericalKMeans\n",
    "    skm = SphericalKMeans(n_clusters=n_clusters)\n",
    "    y_pred_skm = skm.fit_predict(X)\n",
    "    results['SphericalKMeans'] = evaluate_clustering(y, y_pred_skm)\n",
    "\n",
    "    # LSA + KMeans\n",
    "    svd = TruncatedSVD(n_components=7)\n",
    "    lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "    X_lsa = lsa.fit_transform(X)\n",
    "\n",
    "    km_lsa = KMeans(n_clusters=n_clusters, init='k-means++', n_init=10)\n",
    "    y_pred_lsa_km = km_lsa.fit_predict(X_lsa)\n",
    "    results['LSA+KMeans'] = evaluate_clustering(y, y_pred_lsa_km)\n",
    "\n",
    "    return results\n",
    "\n",
    "n_runs = 5\n",
    "aggregated_results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n Running iteration {run + 1}...\")\n",
    "    run_results = run_clustering(\"BBCSport\", bbcsport_X, bbcsport_y, bbcsport_n_clusters)\n",
    "    for method_name, metrics in run_results.items():\n",
    "        for metric_name, value in metrics.items():\n",
    "            aggregated_results[method_name][metric_name].append(value)\n",
    "\n",
    "print(\"\\n Final Aggregated Results:\")\n",
    "for method_name, metric_values in aggregated_results.items():\n",
    "    print(f\"\\n {method_name}\")\n",
    "    for metric_name, values in metric_values.items():\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        print(f\"{metric_name}: Mean = {mean_val:.4f}, Std = {std_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "246af09c-d48d-446a-9d99-bf5d44caab13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KM:\n",
      "Purity: 0.4034\n",
      "NMI: 0.1110\n",
      "ARI: 0.0513\n",
      "\n",
      "SKM:\n",
      "Purity: 0.5949\n",
      "NMI: 0.3060\n",
      "ARI: 0.2850\n",
      "\n",
      "LSAKM:\n",
      "Purity: 0.6695\n",
      "NMI: 0.3770\n",
      "ARI: 0.3874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#AG dataset\n",
    "def run_AG_dataset():\n",
    "    \n",
    "    df = pd.read_csv(\"C:/Users/asus/Downloads/FinalProject/archive (4)/train.csv\", encoding='latin1')\n",
    "\n",
    "    df = df.sample(n=1180, random_state=42)\n",
    "    texts = (df['Title'] + ' ' + df['Description']).astype(str).tolist()\n",
    "    labels = df['Class Index'].tolist()\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_true = label_encoder.fit_transform(labels)\n",
    "    n_clusters = len(np.unique(y_true))\n",
    "\n",
    "    processed_texts = preprocess_text(texts)\n",
    "\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                 min_df=2,\n",
    "                 max_df=0.95,\n",
    "                 token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "   \n",
    "    \n",
    "    X = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    \n",
    "    km = KMeans(n_clusters=n_clusters, init='k-means++')\n",
    "    y_pred = km.fit_predict(X)\n",
    "    results[\"KM\"] = evaluate_clustering(y_true, y_pred)\n",
    "\n",
    "    \n",
    "    skm = SphericalKMeans(n_clusters=n_clusters)\n",
    "    y_pred = skm.fit_predict(X)\n",
    "    results[\"SKM\"] = evaluate_clustering(y_true, y_pred)\n",
    "\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=min(5, X.shape[1]))\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "    X_lsa = lsa.fit_transform(X)\n",
    "    km_lsa = KMeans(n_clusters=n_clusters, init='k-means++')\n",
    "    y_pred = km_lsa.fit_predict(X_lsa)\n",
    "    results[\"LSAKM\"] = evaluate_clustering(y_true, y_pred)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "results = run_AG_dataset()\n",
    "\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e9c598d0-3dea-4e0f-b4b4-18c75fad4930",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Final Aggregated Results:\n",
      "\n",
      " KM\n",
      "Purity: Mean = 0.3939, Std = 0.0600\n",
      "NMI: Mean = 0.0928, Std = 0.0547\n",
      "ARI: Mean = 0.0602, Std = 0.0395\n",
      "\n",
      " SKM\n",
      "Purity: Mean = 0.5661, Std = 0.1110\n",
      "NMI: Mean = 0.2732, Std = 0.1228\n",
      "ARI: Mean = 0.2484, Std = 0.1121\n",
      "\n",
      " LSAKM\n",
      "Purity: Mean = 0.6814, Std = 0.0362\n",
      "NMI: Mean = 0.3960, Std = 0.0326\n",
      "ARI: Mean = 0.4067, Std = 0.0430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "n_runs = 5\n",
    "aggregated_results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n Running iteration {run + 1}...\")\n",
    "    run_results = run_AG_dataset()\n",
    "    for method_name, metrics in run_results.items():\n",
    "        for metric_name, value in metrics.items():\n",
    "            aggregated_results[method_name][metric_name].append(value)\n",
    "\n",
    "print(\"\\n Final Aggregated Results:\")\n",
    "for method_name, metric_values in aggregated_results.items():\n",
    "    print(f\"\\n {method_name}\")\n",
    "    for metric_name, values in metric_values.items():\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        print(f\"{metric_name}: Mean = {mean_val:.4f}, Std = {std_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "985eccdb-a1e8-4562-9613-e63c5d88a709",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Cornell.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Texas.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Washington.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Wisconsin.csv...\n",
      "\n",
      "=== Final Results ===\n",
      "\n",
      "Dataset: Cornell\n",
      "  KM:\n",
      "    Purity: 0.4974\n",
      "    NMI: 0.1517\n",
      "    ARI: 0.0869\n",
      "  SKM:\n",
      "    Purity: 0.6754\n",
      "    NMI: 0.3468\n",
      "    ARI: 0.2454\n",
      "  LSAKM:\n",
      "    Purity: 0.6545\n",
      "    NMI: 0.3534\n",
      "    ARI: 0.3402\n",
      "\n",
      "Dataset: Texas\n",
      "  KM:\n",
      "    Purity: 0.5668\n",
      "    NMI: 0.1233\n",
      "    ARI: 0.1482\n",
      "  SKM:\n",
      "    Purity: 0.6631\n",
      "    NMI: 0.2494\n",
      "    ARI: 0.1682\n",
      "  LSAKM:\n",
      "    Purity: 0.7112\n",
      "    NMI: 0.3703\n",
      "    ARI: 0.2740\n",
      "\n",
      "Dataset: Washington\n",
      "  KM:\n",
      "    Purity: 0.6638\n",
      "    NMI: 0.3156\n",
      "    ARI: 0.2777\n",
      "  SKM:\n",
      "    Purity: 0.6812\n",
      "    NMI: 0.2806\n",
      "    ARI: 0.3069\n",
      "  LSAKM:\n",
      "    Purity: 0.7293\n",
      "    NMI: 0.3776\n",
      "    ARI: 0.3296\n",
      "\n",
      "Dataset: Wisconsin\n",
      "  KM:\n",
      "    Purity: 0.6566\n",
      "    NMI: 0.2904\n",
      "    ARI: 0.1724\n",
      "  SKM:\n",
      "    Purity: 0.6792\n",
      "    NMI: 0.2966\n",
      "    ARI: 0.2953\n",
      "  LSAKM:\n",
      "    Purity: 0.6906\n",
      "    NMI: 0.3569\n",
      "    ARI: 0.3090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def run_experiments_webkb_classic(folder_path):\n",
    "    results = {}\n",
    "\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            print(f\"\\nProcessing {file}...\")\n",
    "\n",
    "            df = pd.read_csv(file_path)\n",
    "            texts = df['raw_text'].astype(str).tolist()\n",
    "            labels = df['label'].tolist()\n",
    "            y_true = LabelEncoder().fit_transform(labels)\n",
    "            n_clusters = len(np.unique(y_true))\n",
    "\n",
    "            \n",
    "            processed_texts = preprocess_text(texts)\n",
    "\n",
    "            vectorizer = TfidfVectorizer(\n",
    "                stop_words='english',\n",
    "                min_df=2,\n",
    "                max_df=0.95,\n",
    "                token_pattern=r'\\b[a-zA-Z]{3,}\\b'\n",
    "            )\n",
    "            X = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "        \n",
    "            dataset_results = {}\n",
    "\n",
    "            km = KMeans(n_clusters=n_clusters, init='k-means++')\n",
    "            y_pred = km.fit_predict(X)\n",
    "            dataset_results[\"KM\"] = evaluate_clustering(y_true, y_pred)\n",
    "\n",
    "            skm = SphericalKMeans(n_clusters=n_clusters)\n",
    "            y_pred = skm.fit_predict(X)\n",
    "            dataset_results[\"SKM\"] = evaluate_clustering(y_true, y_pred)\n",
    "\n",
    "            svd = TruncatedSVD(n_components=min(15, X.shape[1]))\n",
    "            normalizer = Normalizer(copy=False)\n",
    "            lsa = make_pipeline(svd, normalizer)\n",
    "            X_lsa = lsa.fit_transform(X)\n",
    "            km_lsa = KMeans(n_clusters=n_clusters, init='k-means++')\n",
    "            y_pred = km_lsa.fit_predict(X_lsa)\n",
    "            dataset_results[\"LSAKM\"] = evaluate_clustering(y_true, y_pred)\n",
    "\n",
    "            results[file.replace(\".csv\", \"\")] = dataset_results\n",
    "\n",
    "    return results\n",
    "\n",
    "results = run_experiments_webkb_classic(\"C:/Users/asus/Downloads/FinalProject/WebKB\")\n",
    "\n",
    "print(\"\\n=== Final Results ===\")\n",
    "for dataset, algos in results.items():\n",
    "    print(f\"\\nDataset: {dataset}\")\n",
    "    for algo_name, metrics in algos.items():\n",
    "        print(f\"  {algo_name}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"    {metric}: {value:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2d79ecc1-dbe5-4bd2-aa06-97be19ade8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 1...\n",
      "\n",
      "Processing Cornell.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Texas.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Washington.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Wisconsin.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 2...\n",
      "\n",
      "Processing Cornell.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Texas.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Washington.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Wisconsin.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 3...\n",
      "\n",
      "Processing Cornell.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Texas.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Washington.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Wisconsin.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 4...\n",
      "\n",
      "Processing Cornell.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Texas.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Washington.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Wisconsin.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 5...\n",
      "\n",
      "Processing Cornell.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Texas.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Washington.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Wisconsin.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Aggregated Results:\n",
      "\n",
      "=== Cornell ===\n",
      "\n",
      "Method: KM\n",
      "Purity: Mean = 0.6168, Std = 0.0493\n",
      "NMI: Mean = 0.3071, Std = 0.0602\n",
      "ARI: Mean = 0.2069, Std = 0.0621\n",
      "\n",
      "Method: SKM\n",
      "Purity: Mean = 0.6461, Std = 0.0240\n",
      "NMI: Mean = 0.3173, Std = 0.0397\n",
      "ARI: Mean = 0.2366, Std = 0.0333\n",
      "\n",
      "Method: LSAKM\n",
      "Purity: Mean = 0.6712, Std = 0.0130\n",
      "NMI: Mean = 0.3809, Std = 0.0286\n",
      "ARI: Mean = 0.3389, Std = 0.0625\n",
      "\n",
      "=== Texas ===\n",
      "\n",
      "Method: KM\n",
      "Purity: Mean = 0.6257, Std = 0.0251\n",
      "NMI: Mean = 0.1910, Std = 0.0450\n",
      "ARI: Mean = 0.1749, Std = 0.0662\n",
      "\n",
      "Method: SKM\n",
      "Purity: Mean = 0.6727, Std = 0.0520\n",
      "NMI: Mean = 0.2316, Std = 0.0940\n",
      "ARI: Mean = 0.2235, Std = 0.1234\n",
      "\n",
      "Method: LSAKM\n",
      "Purity: Mean = 0.6877, Std = 0.0307\n",
      "NMI: Mean = 0.3354, Std = 0.0440\n",
      "ARI: Mean = 0.2279, Std = 0.0464\n",
      "\n",
      "=== Washington ===\n",
      "\n",
      "Method: KM\n",
      "Purity: Mean = 0.6568, Std = 0.0446\n",
      "NMI: Mean = 0.2801, Std = 0.0638\n",
      "ARI: Mean = 0.2540, Std = 0.0977\n",
      "\n",
      "Method: SKM\n",
      "Purity: Mean = 0.6934, Std = 0.0160\n",
      "NMI: Mean = 0.3109, Std = 0.0400\n",
      "ARI: Mean = 0.2733, Std = 0.0295\n",
      "\n",
      "Method: LSAKM\n",
      "Purity: Mean = 0.7127, Std = 0.0145\n",
      "NMI: Mean = 0.3728, Std = 0.0366\n",
      "ARI: Mean = 0.3764, Std = 0.0360\n",
      "\n",
      "=== Wisconsin ===\n",
      "\n",
      "Method: KM\n",
      "Purity: Mean = 0.6777, Std = 0.0158\n",
      "NMI: Mean = 0.3238, Std = 0.0268\n",
      "ARI: Mean = 0.3081, Std = 0.0405\n",
      "\n",
      "Method: SKM\n",
      "Purity: Mean = 0.6611, Std = 0.0454\n",
      "NMI: Mean = 0.3069, Std = 0.0621\n",
      "ARI: Mean = 0.2678, Std = 0.0639\n",
      "\n",
      "Method: LSAKM\n",
      "Purity: Mean = 0.6755, Std = 0.0267\n",
      "NMI: Mean = 0.3211, Std = 0.0456\n",
      "ARI: Mean = 0.2644, Std = 0.0658\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "n_runs = 5\n",
    "aggregated_results = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n Running iteration {run + 1}...\")\n",
    "    run_results = run_experiments_webkb_classic(\"C:/Users/asus/Downloads/FinalProject/WebKB\")\n",
    "\n",
    "    \n",
    "    for dataset_name, methods in run_results.items():\n",
    "        for method_name, metrics in methods.items():\n",
    "            for metric_name, value in metrics.items():\n",
    "                aggregated_results[dataset_name][method_name][metric_name].append(value)\n",
    "\n",
    "print(\"\\nFinal Aggregated Results:\")\n",
    "for dataset_name, methods in aggregated_results.items():\n",
    "    print(f\"\\n=== {dataset_name} ===\")\n",
    "    for method_name, metrics in methods.items():\n",
    "        print(f\"\\nMethod: {method_name}\")\n",
    "        for metric_name, values in metrics.items():\n",
    "            mean_val = np.mean(values)\n",
    "            std_val = np.std(values)\n",
    "            print(f\"{metric_name}: Mean = {mean_val:.4f}, Std = {std_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bfc4a181-2ed2-4c8a-a394-0aacb6e0ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "import pyswarms as ps\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "\n",
    "\n",
    "class ImprovedGAKM_NMI:\n",
    "    def __init__(self, n_clusters, y_true, pop_size=20, n_gen=50, mutation_rate=0.2):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.pop_size = pop_size\n",
    "        self.n_gen = n_gen\n",
    "        self.y_true = y_true\n",
    "        self.mutation_rate = mutation_rate\n",
    "\n",
    "    def _evaluate(self, centers, X):\n",
    "        \n",
    "        distances = np.array([np.linalg.norm(X - center, axis=1) for center in centers])\n",
    "        labels = np.argmin(distances, axis=0)\n",
    "        \n",
    "        \n",
    "        if len(np.unique(labels)) < self.n_clusters:\n",
    "            return -1  \n",
    "        return normalized_mutual_info_score(self.y_true, labels)\n",
    "\n",
    "    def fit_predict(self, X):\n",
    "        X = X.toarray() if hasattr(X, 'toarray') else X\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        \n",
    "        population = []\n",
    "        for _ in range(self.pop_size):\n",
    "            indices = np.random.choice(n_samples, self.n_clusters, replace=False)\n",
    "            centers = X[indices]\n",
    "            population.append(centers)\n",
    "\n",
    "        best_centers = None\n",
    "        best_score = -1\n",
    "\n",
    "        for gen in range(self.n_gen):\n",
    "            scores = [self._evaluate(centers, X) for centers in population]\n",
    "            sorted_indices = np.argsort(scores)[::-1]\n",
    "            population = [population[i] for i in sorted_indices]\n",
    "\n",
    "            if scores[sorted_indices[0]] > best_score:\n",
    "                best_score = scores[sorted_indices[0]]\n",
    "                best_centers = population[0]\n",
    "\n",
    "            \n",
    "            parents = population[:self.pop_size // 2]\n",
    "            new_population = parents.copy()\n",
    "\n",
    "            \n",
    "            while len(new_population) < self.pop_size:\n",
    "                p1, p2 = random.sample(parents, 2)\n",
    "                #child = (parent1 + parent2) / 2\n",
    "                crossover_point = np.random.randint(1, self.n_clusters)\n",
    "                child = np.vstack((p1[:crossover_point], p2[crossover_point:]))\n",
    "\n",
    "                \n",
    "                if random.random() < self.mutation_rate:\n",
    "                    noise = np.random.normal(0, 0.1, child.shape)\n",
    "                    child = np.clip(child + noise, 0, 1)  \n",
    "\n",
    "                new_population.append(child)\n",
    "\n",
    "            population = new_population\n",
    "\n",
    "        \n",
    "        final_distances = np.array([np.linalg.norm(X - center, axis=1) for center in best_centers])\n",
    "        return np.argmin(final_distances, axis=0)\n",
    "\n",
    "\n",
    "class ImprovedGAKM:\n",
    "    def __init__(self, n_clusters, pop_size=15, n_gen=30):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.pop_size = pop_size\n",
    "        self.n_gen = n_gen\n",
    "        \n",
    "    def fit_predict(self, X):\n",
    "        X = X.toarray() if hasattr(X, 'toarray') else X\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        \n",
    "        if self.n_clusters == 1:\n",
    "            return np.zeros(n_samples, dtype=int)\n",
    "        \n",
    "        \n",
    "        population = []\n",
    "        for _ in range(self.pop_size):\n",
    "            centers = X[np.random.choice(n_samples, self.n_clusters, replace=False)]\n",
    "            population.append(centers)\n",
    "        \n",
    "        best_centers = None\n",
    "        best_score = -np.inf\n",
    "        \n",
    "        for _ in range(self.n_gen):\n",
    "            \n",
    "            scores = []\n",
    "            for centers in population:\n",
    "                distances = np.array([np.linalg.norm(X - center, axis=1) for center in centers])\n",
    "                labels = np.argmin(distances, axis=0)\n",
    "                \n",
    "                if len(np.unique(labels)) == self.n_clusters:\n",
    "                    score = silhouette_score(X, labels)\n",
    "                    scores.append(score)\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_centers = centers\n",
    "                else:\n",
    "                    scores.append(-np.inf)\n",
    "            \n",
    "            \n",
    "            selected_indices = np.argsort(scores)[-self.pop_size//2:]\n",
    "            new_population = [population[i] for i in selected_indices]\n",
    "            \n",
    "           \n",
    "            while len(new_population) < self.pop_size:\n",
    "                parent1, parent2 = random.sample(new_population, 2)\n",
    "                #child = (parent1 + parent2) / 2  \n",
    "                crossover_point = np.random.randint(1, self.n_clusters)\n",
    "                child = np.vstack((\n",
    "                    parent1[:crossover_point],\n",
    "                    parent2[crossover_point:]\n",
    "                ))\n",
    "                \n",
    "                if random.random() < 0.2:\n",
    "                    child += np.random.normal(0, 0.5, size=child.shape)\n",
    "                \n",
    "                new_population.append(child)\n",
    "            \n",
    "            population = new_population\n",
    "        \n",
    "        \n",
    "        distances = np.array([np.linalg.norm(X - center, axis=1) for center in best_centers])\n",
    "        return np.argmin(distances, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cea32955-e734-4866-9ecb-167457011764",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Computer category ===\n",
      "Running GAKM...\n",
      "GAKM completed for Computer\n",
      "\n",
      "=== Processing Politics category ===\n",
      "Running GAKM...\n",
      "GAKM completed for Politics\n",
      "\n",
      "=== Processing Miscellaneous category ===\n",
      "Running GAKM...\n",
      "GAKM completed for Miscellaneous\n",
      "\n",
      "=== Processing Religion category ===\n",
      "Running GAKM...\n",
      "GAKM completed for Religion\n",
      "\n",
      "=== Processing Science category ===\n",
      "Running GAKM...\n",
      "GAKM completed for Science\n",
      "\n",
      "=== Final Results ===\n",
      "\n",
      "Computer_GAKM:\n",
      "Purity: 0.3682\n",
      "NMI: 0.0778\n",
      "ARI: 0.0549\n",
      "\n",
      "Politics_GAKM:\n",
      "Purity: 0.5337\n",
      "NMI: 0.1119\n",
      "ARI: 0.1156\n",
      "\n",
      "Miscellaneous_GAKM:\n",
      "Purity: 0.5822\n",
      "NMI: 0.2308\n",
      "ARI: 0.2310\n",
      "\n",
      "Religion_GAKM:\n",
      "Purity: 0.4719\n",
      "NMI: 0.0338\n",
      "ARI: 0.0443\n",
      "\n",
      "Science_GAKM:\n",
      "Purity: 0.4643\n",
      "NMI: 0.1082\n",
      "ARI: 0.1011\n"
     ]
    }
   ],
   "source": [
    "#20newsgroup\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "import pyswarms as ps\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_experiments():\n",
    "    categories = {\n",
    "        'Computer': ['comp.graphics', 'comp.os.ms-windows.misc', \n",
    "                    'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', \n",
    "                    'comp.windows.x'],\n",
    "        'Politics': ['talk.politics.misc', 'talk.politics.guns', \n",
    "                    'talk.politics.mideast'],\n",
    "        'Miscellaneous': ['misc.forsale','talk.politics.misc','talk.religion.misc','comp.os.ms-windows.misc'],\n",
    "        'Religion': ['talk.religion.misc', 'alt.atheism', 'soc.religion.christian'],\n",
    "        'Science': ['sci.crypt', 'sci.electronics', 'sci.med', 'sci.space']\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, subcats in categories.items():\n",
    "        print(f\"\\n=== Processing {name} category ===\")\n",
    "        dataset = fetch_20newsgroups(subset='all', categories=subcats, \n",
    "                                   remove=('headers', 'footers', 'quotes'))\n",
    "        n_clusters = len(subcats)\n",
    "        \n",
    "        processed_texts = preprocess_text(dataset.data)\n",
    "\n",
    "\n",
    "        vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                     min_df=2,\n",
    "                     max_df=0.95,\n",
    "                     token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "   \n",
    "    \n",
    "        X = vectorizer.fit_transform(processed_texts)\n",
    "        y_true = dataset.target\n",
    "        \n",
    "        \n",
    "        algorithms = {\n",
    "\n",
    "            'GAKM': ImprovedGAKM_NMI(n_clusters=n_clusters, y_true=y_true)\n",
    "            \n",
    "            \n",
    "            \n",
    "        }\n",
    "        \n",
    "        for algo_name, algo in algorithms.items():\n",
    "            try:\n",
    "                print(f\"Running {algo_name}...\")\n",
    "                y_pred = algo.fit_predict(X)\n",
    "                results[f\"{name}_{algo_name}\"] = evaluate_clustering(y_true, y_pred)\n",
    "                print(f\"{algo_name} completed for {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {algo_name} for {name}: {str(e)}\")\n",
    "                results[f\"{name}_{algo_name}\"] = {'Purity': 0, 'NMI': 0, 'ARI': 0}\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_experiments()\n",
    "    print(\"\\n=== Final Results ===\")\n",
    "    for name, metrics in results.items():\n",
    "        print(f\"\\n{name}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60cf2a8b-16d6-4eb6-9708-22cbf1e9f81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GAKM...\n",
      "GAKM completed\n",
      "\n",
      "=== Final Results ===\n",
      "\n",
      "GAKM:\n",
      "Purity: 0.6643\n",
      "NMI: 0.4178\n",
      "ARI: 0.4109\n"
     ]
    }
   ],
   "source": [
    "#BBC news\n",
    "def run_experiments_BBC_dataset():\n",
    "    \n",
    "    df = pd.read_csv(\"C:/Users/asus/Downloads/FinalProject/dataset.csv\", encoding='latin1')\n",
    "\n",
    "    texts = df['news'].astype(str).tolist()\n",
    "    labels = df['type'].astype(str).tolist()\n",
    "    y_true = LabelEncoder().fit_transform(labels)\n",
    "    n_clusters = len(np.unique(y_true))\n",
    "\n",
    "    processed_texts = preprocess_text(texts)\n",
    "\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                 min_df=2,\n",
    "                 max_df=0.95,\n",
    "                 token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "   \n",
    "    \n",
    "    X = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    algorithms = {\n",
    "        'GAKM': ImprovedGAKM_NMI(n_clusters=n_clusters, y_true=y_true)\n",
    "            \n",
    "    }\n",
    "        \n",
    "    for algo_name, algo in algorithms.items():\n",
    "        try:\n",
    "            print(f\"Running {algo_name}...\")\n",
    "            y_pred = algo.fit_predict(X)\n",
    "            results[f\"{algo_name}\"] = evaluate_clustering(y_true, y_pred)\n",
    "            print(f\"{algo_name} completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {algo_name} : {str(e)}\")\n",
    "            results[f\"{algo_name}\"] = {'Purity': 0, 'NMI': 0, 'ARI': 0}\n",
    "    \n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "results = run_experiments_BBC_dataset()\n",
    "\n",
    "print(\"\\n=== Final Results ===\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b76b675e-84d5-4dfe-8bc4-331eda1852de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GAKM...\n",
      "GAKM completed\n",
      "\n",
      "=== Final Results ===\n",
      "\n",
      "GAKM:\n",
      "Purity: 0.8659\n",
      "NMI: 0.1133\n",
      "ARI: 0.2713\n"
     ]
    }
   ],
   "source": [
    "#SMS\n",
    "df = pd.read_csv(\"C:/Users/asus/Downloads/FinalProject/archive (3)/spam.csv\", encoding='latin1')[['v1', 'v2']]\n",
    "df.columns = ['label', 'text']\n",
    "\n",
    "processed_texts = preprocess_text(df['text'].tolist())\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english',\n",
    "             min_df=2,\n",
    "             max_df=0.95,\n",
    "             token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "\n",
    "X = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "label_mapping = {'ham': 0, 'spam': 1}\n",
    "df['label'] = df['label'].map(label_mapping)\n",
    "\n",
    "y_true = df['label'].tolist()\n",
    "\n",
    "n_clusters = len(np.unique(y_true))\n",
    "\n",
    "results = {}\n",
    "\n",
    "algorithms = {\n",
    "    'GAKM': ImprovedGAKM_NMI(n_clusters=n_clusters, y_true=y_true)\n",
    "            \n",
    "}\n",
    "        \n",
    "for algo_name, algo in algorithms.items():\n",
    "    try:\n",
    "        print(f\"Running {algo_name}...\")\n",
    "        y_pred = algo.fit_predict(X)\n",
    "        results[f\"{algo_name}\"] = evaluate_clustering(y_true, y_pred)\n",
    "        print(f\"{algo_name} completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {algo_name} : {str(e)}\")\n",
    "        results[f\"{algo_name}\"] = {'Purity': 0, 'NMI': 0, 'ARI': 0}\n",
    "    \n",
    "\n",
    "print(\"\\n=== Final Results ===\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7368ce1-8201-42ed-87e2-05835f07a396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_2168\\3475525762.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=min(N, len(x)), random_state=42))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GAKM...\n",
      "GAKM completed\n",
      "\n",
      "=== Final Results ===\n",
      "\n",
      "GAKM:\n",
      "Purity: 0.2100\n",
      "NMI: 0.0932\n",
      "ARI: 0.0329\n"
     ]
    }
   ],
   "source": [
    "#DMOZ dataset\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/asus/Downloads/FinalProject/archive (5)/dmoz.csv\", encoding='latin1')\n",
    "df = df.rename(columns={\n",
    "            'category': 'Class Index',\n",
    "            'title': 'Title',\n",
    "            'desc': 'Description'\n",
    "        })\n",
    "N = 300  \n",
    "df_balanced = (\n",
    "    df.groupby('Class Index', group_keys=False)\n",
    "    .apply(lambda x: x.sample(n=min(N, len(x)), random_state=42))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n",
    "texts = (df_balanced['Title'] + ' ' + df_balanced['Description']).astype(str).tolist()\n",
    "labels = df_balanced['Class Index'].tolist()\n",
    "\n",
    "\n",
    "processed_texts = preprocess_text(texts)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english',\n",
    "             min_df=2,\n",
    "             max_df=0.95,\n",
    "             token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "\n",
    "X = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(labels)\n",
    "\n",
    "\n",
    "\n",
    "n_clusters = len(np.unique(y_true))\n",
    "\n",
    "results = {}\n",
    "\n",
    "algorithms = {\n",
    "    'GAKM': ImprovedGAKM_NMI(n_clusters=n_clusters, y_true=y_true)\n",
    "            \n",
    "}\n",
    "        \n",
    "for algo_name, algo in algorithms.items():\n",
    "    try:\n",
    "        print(f\"Running {algo_name}...\")\n",
    "        y_pred = algo.fit_predict(X)\n",
    "        results[f\"{algo_name}\"] = evaluate_clustering(y_true, y_pred)\n",
    "        print(f\"{algo_name} completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {algo_name} : {str(e)}\")\n",
    "        results[f\"{algo_name}\"] = {'Purity': 0, 'NMI': 0, 'ARI': 0}\n",
    "    \n",
    "\n",
    "print(\"\\n=== Final Results ===\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f7a1173-33e3-4326-9f0d-ac48dede4c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results on BBCSport Dataset ===\n",
      "Running GAKM...\n",
      "GAKM completed\n",
      "\n",
      "GAKM:\n",
      "Purity: 0.6133\n",
      "NMI: 0.3077\n",
      "ARI: 0.2559\n"
     ]
    }
   ],
   "source": [
    "#BBC Sport\n",
    "def run_GAKM_BBCS(name, X, y_true, n_clusters):\n",
    "    print(f\"\\n=== Results on {name} Dataset ===\")\n",
    "    results = {}\n",
    "    algorithms = {\n",
    "        'GAKM': ImprovedGAKM_NMI(n_clusters=n_clusters, y_true=y_true)\n",
    "            \n",
    "    }\n",
    "        \n",
    "    for algo_name, algo in algorithms.items():\n",
    "        try:\n",
    "            print(f\"Running {algo_name}...\")\n",
    "            y_pred = algo.fit_predict(X)\n",
    "            results[f\"{algo_name}\"] = evaluate_clustering(y_true, y_pred)\n",
    "            print(f\"{algo_name} completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {algo_name} : {str(e)}\")\n",
    "            results[f\"{algo_name}\"] = {'Purity': 0, 'NMI': 0, 'ARI': 0}\n",
    "   \n",
    "    \n",
    "\n",
    "    for method, metrics in results.items():\n",
    "        print(f\"\\n{method}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "\n",
    "bbcsport_X = pd.read_csv(r'C:\\Users\\asus\\Downloads\\FinalProject\\archive (2)\\bbcsport_mtx.csv', header=None).values\n",
    "bbcsport_y = pd.read_csv(r'C:\\Users\\asus\\Downloads\\FinalProject\\archive (2)\\bbcsport_classes.csv', header=None)[0].values\n",
    "bbcsport_n_clusters = len(np.unique(bbcsport_y))\n",
    "\n",
    "run_GAKM_BBCS(\"BBCSport\", bbcsport_X, bbcsport_y, bbcsport_n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b62e824-24d7-46f3-aff6-bde2ac0afda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_2168\\3422350769.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('Class Index', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GAKM...\n",
      "GAKM completed\n",
      "\n",
      "GAKM:\n",
      "Purity: 0.4415\n",
      "NMI: 0.1009\n",
      "ARI: 0.0767\n"
     ]
    }
   ],
   "source": [
    "#AG dataset\n",
    "def run_AG_dataset():\n",
    "    \n",
    "    df = pd.read_csv(\"C:/Users/asus/Downloads/FinalProject/archive (4)/train.csv\", encoding='latin1')\n",
    "\n",
    "    class_counts = df['Class Index'].value_counts()\n",
    "    min_class_count = class_counts.min()  \n",
    "\n",
    "    \n",
    "    samples_per_class = min(295, min_class_count)\n",
    "\n",
    "    \n",
    "    df = df.groupby('Class Index', group_keys=False).apply(\n",
    "        lambda x: x.sample(n=samples_per_class, random_state=42)\n",
    "    )\n",
    "    texts = (df['Title'] + ' ' + df['Description']).astype(str).tolist()\n",
    "    labels = df['Class Index'].tolist()\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_true = label_encoder.fit_transform(labels)\n",
    "    n_clusters = len(np.unique(y_true))\n",
    "\n",
    "    processed_texts = preprocess_text(texts)\n",
    "\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                 min_df=2,\n",
    "                 max_df=0.95,\n",
    "                 token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "   \n",
    "    \n",
    "    X = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    algorithms = {\n",
    "        'GAKM': ImprovedGAKM_NMI(n_clusters=n_clusters, y_true=y_true)\n",
    "            \n",
    "    }\n",
    "        \n",
    "    for algo_name, algo in algorithms.items():\n",
    "        try:\n",
    "            print(f\"Running {algo_name}...\")\n",
    "            y_pred = algo.fit_predict(X)\n",
    "            results[f\"{algo_name}\"] = evaluate_clustering(y_true, y_pred)\n",
    "            print(f\"{algo_name} completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {algo_name} : {str(e)}\")\n",
    "            results[f\"{algo_name}\"] = {'Purity': 0, 'NMI': 0, 'ARI': 0}\n",
    "    \n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "results = run_AG_dataset()\n",
    "\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d3e1c28b-b945-4f13-bd40-ca07347f74c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Cornell.csv...\n",
      "Running GAKM on Cornell.csv...\n",
      "SCPSO completed on Cornell.csv\n",
      "\n",
      "Processing Texas.csv...\n",
      "Running GAKM on Texas.csv...\n",
      "SCPSO completed on Texas.csv\n",
      "\n",
      "Processing Washington.csv...\n",
      "Running GAKM on Washington.csv...\n",
      "SCPSO completed on Washington.csv\n",
      "\n",
      "Processing Wisconsin.csv...\n",
      "Running GAKM on Wisconsin.csv...\n",
      "SCPSO completed on Wisconsin.csv\n",
      "\n",
      "=== Final Results on WebKB ===\n",
      "\n",
      "Cornell:\n",
      "Purity: 0.6963\n",
      "NMI: 0.3588\n",
      "ARI: 0.3216\n",
      "\n",
      "Texas:\n",
      "Purity: 0.7112\n",
      "NMI: 0.3225\n",
      "ARI: 0.4426\n",
      "\n",
      "Washington:\n",
      "Purity: 0.6987\n",
      "NMI: 0.2782\n",
      "ARI: 0.2115\n",
      "\n",
      "Wisconsin:\n",
      "Purity: 0.7170\n",
      "NMI: 0.3202\n",
      "ARI: 0.3210\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def run_experiments_webkb_dataset(webkb_folder_path):\n",
    "    results = {}\n",
    "    \n",
    "\n",
    "    for file in os.listdir(webkb_folder_path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(webkb_folder_path, file)\n",
    "            print(f\"\\nProcessing {file}...\")\n",
    "\n",
    "            df = pd.read_csv(file_path)\n",
    "            texts = df['raw_text'].astype(str).tolist()\n",
    "            labels = df['label'].tolist()\n",
    "            y_true = LabelEncoder().fit_transform(labels)\n",
    "            n_clusters = len(np.unique(y_true))\n",
    "\n",
    "            processed_texts = preprocess_text(texts)\n",
    "\n",
    "            \n",
    "            vectorizer = TfidfVectorizer(\n",
    "                stop_words='english',\n",
    "                min_df=2,\n",
    "                max_df=0.95,\n",
    "                token_pattern=r'\\b[a-zA-Z]{3,}\\b'\n",
    "            )\n",
    "            X = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "            try:\n",
    "                algo = ImprovedGAKM_NMI(n_clusters=n_clusters, y_true=y_true)\n",
    "                print(f\"Running GAKM on {file}...\")\n",
    "                y_pred = algo.fit_predict(X)\n",
    "                metrics = evaluate_clustering(y_true, y_pred)\n",
    "                print(f\"SCPSO completed on {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in GAKM on {file}: {str(e)}\")\n",
    "                metrics = {'Purity': 0, 'NMI': 0, 'ARI': 0}\n",
    "\n",
    "            \n",
    "            dataset_name = file.replace(\".csv\", \"\")\n",
    "            results[dataset_name] = metrics\n",
    "\n",
    "    return results\n",
    "\n",
    "results = run_experiments_webkb_dataset(\"C:/Users/asus/Downloads/FinalProject/WebKB\")\n",
    "\n",
    "print(\"\\n=== Final Results on WebKB ===\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "66571451-ac4e-43d7-be9f-1e8c8b31ce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_kernels\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "class SCPSO:\n",
    "    def __init__(self, n_clusters=4, n_particles=20, max_iter=100, w=0.7, c1=1.5, c2=1.5, gamma=0.01):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_particles = n_particles\n",
    "        self.max_iter = max_iter\n",
    "        self.w = w\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def _compute_laplacian(self, X):\n",
    "        \n",
    "        W = rbf_kernel(X, gamma=self.gamma)\n",
    "        D = np.diag(np.sum(W, axis=1))\n",
    "        D_inv_sqrt = np.linalg.inv(np.sqrt(D))\n",
    "        L = np.eye(len(X)) - D_inv_sqrt @ W @ D_inv_sqrt\n",
    "        return L\n",
    "\n",
    "    def _compute_embedding(self, L):\n",
    "        \n",
    "        eigvals, eigvecs = np.linalg.eig(L)\n",
    "        idx = np.argsort(eigvals)\n",
    "        embedding = eigvecs[:, idx[1:self.n_clusters+1]].real\n",
    "        return normalize(embedding)\n",
    "\n",
    "    def _pso_init(self, X_embedding):\n",
    "        \n",
    "        n_features = X_embedding.shape[1]\n",
    "        particles = np.random.rand(self.n_particles, self.n_clusters, n_features)\n",
    "        velocities = np.zeros_like(particles)\n",
    "        pbest = particles.copy()\n",
    "        pbest_scores = np.full(self.n_particles, -np.inf)\n",
    "        return particles, velocities, pbest, pbest_scores\n",
    "\n",
    "    def _compute_fitness(self, X_embedding, centroids):\n",
    "        \n",
    "        distances = np.zeros((X_embedding.shape[0], self.n_clusters))\n",
    "        for i in range(self.n_clusters):\n",
    "            distances[:, i] = np.linalg.norm(X_embedding - centroids[i], axis=1)\n",
    "        \n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        \n",
    "        if len(np.unique(labels)) < self.n_clusters:\n",
    "            return -np.inf\n",
    "\n",
    "        \n",
    "        intra_dist = np.mean([np.mean(distances[labels == i, i]) for i in range(self.n_clusters)])\n",
    "        \n",
    "        \n",
    "        inter_distances = []\n",
    "        for i in range(self.n_clusters):\n",
    "            mask = labels == i\n",
    "            if np.sum(mask) == 0:\n",
    "                return -np.inf\n",
    "            min_dist = np.min([np.mean(distances[mask, j]) for j in range(self.n_clusters) if j != i] or [np.inf])\n",
    "            inter_distances.append(min_dist)\n",
    "        \n",
    "        inter_dist = np.mean(inter_distances)\n",
    "        return inter_dist / (intra_dist + 1e-10)\n",
    "\n",
    "    def _pso_optimize(self, X_embedding):\n",
    "        \n",
    "        particles, velocities, pbest, pbest_scores = self._pso_init(X_embedding)\n",
    "        gbest = pbest[0].copy()\n",
    "        gbest_score = -np.inf\n",
    "        \n",
    "        for _ in range(self.max_iter):\n",
    "            for i in range(self.n_particles):\n",
    "                current_score = self._compute_fitness(X_embedding, particles[i])\n",
    "                \n",
    "                if current_score > pbest_scores[i]:\n",
    "                    pbest_scores[i] = current_score\n",
    "                    pbest[i] = particles[i].copy()\n",
    "                \n",
    "                if current_score > gbest_score:\n",
    "                    gbest_score = current_score\n",
    "                    gbest = particles[i].copy()\n",
    "            \n",
    "            for i in range(self.n_particles):\n",
    "                r1, r2 = np.random.rand(2)\n",
    "                velocities[i] = (self.w * velocities[i] + \n",
    "                                self.c1 * r1 * (pbest[i] - particles[i]) + \n",
    "                                self.c2 * r2 * (gbest - particles[i]))\n",
    "                particles[i] += velocities[i]\n",
    "        \n",
    "        return gbest\n",
    "\n",
    "    def fit_predict(self, X):\n",
    "        \n",
    "        if hasattr(X, 'toarray'):\n",
    "            X = X.toarray()\n",
    "        \n",
    "        L = self._compute_laplacian(X)\n",
    "        X_embedding = self._compute_embedding(L)\n",
    "        centroids = self._pso_optimize(X_embedding)\n",
    "        \n",
    "        \n",
    "        distances = np.zeros((X_embedding.shape[0], self.n_clusters))\n",
    "        for i in range(self.n_clusters):\n",
    "            distances[:, i] = np.linalg.norm(X_embedding - centroids[i], axis=1)\n",
    "        \n",
    "        return np.argmin(distances, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5f8d8a6b-ca53-4f5a-99a3-e7e0f1bfd922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm  \n",
    "\n",
    "def find_optimal_gamma(X, y_true, n_clusters, gamma_range):\n",
    "   \n",
    "    best_nmi = -1\n",
    "    best_gamma = None\n",
    "    \n",
    "    for gamma in tqdm(gamma_range, desc='Finding gamma'):\n",
    "        try:\n",
    "            model = SCPSO(n_clusters=n_clusters, gamma=gamma)\n",
    "            y_pred = model.fit_predict(X)\n",
    "            \n",
    "            \n",
    "            current_nmi = normalized_mutual_info_score(y_true, y_pred)\n",
    "            \n",
    "            if current_nmi > best_nmi:\n",
    "                best_nmi = current_nmi\n",
    "                best_gamma = gamma\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return best_gamma, best_nmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01740de-e81d-4266-8831-dc472bb9bd44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c620206e-d5ad-4b2a-a263-26aa263ad8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_6200\\3812536144.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('Class Index', group_keys=False).apply(\n",
      "Finding gamma: 100%|| 20/20 [01:44<00:00,  5.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best gamma: 0.33598\n",
      "Running SCPSO...\n",
      "SCPSO completed\n",
      "\n",
      "SCPSO:\n",
      "Purity: 0.6483\n",
      "NMI: 0.3835\n",
      "ARI: 0.4042\n"
     ]
    }
   ],
   "source": [
    "def run_AG_dataset():\n",
    "    \n",
    "    df = pd.read_csv(\"C:/Users/asus/Downloads/FinalProject/archive (4)/train.csv\", encoding='latin1')\n",
    "    \n",
    "    class_counts = df['Class Index'].value_counts()\n",
    "    min_class_count = class_counts.min()  \n",
    "    samples_per_class = min(295, min_class_count)\n",
    "    \n",
    "    df = df.groupby('Class Index', group_keys=False).apply(\n",
    "        lambda x: x.sample(n=samples_per_class, random_state=42)\n",
    "    )\n",
    "    \n",
    "    texts = (df['Title'] + ' ' + df['Description']).astype(str).tolist()\n",
    "    labels = df['Class Index'].tolist()\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_true = label_encoder.fit_transform(labels)\n",
    "    n_clusters = len(np.unique(y_true))\n",
    "\n",
    "    \n",
    "    processed_texts = preprocess_text(texts)\n",
    "    \n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                 min_df=2,\n",
    "                 max_df=0.95,\n",
    "                 token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "    X = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "    gamma_candidates = np.logspace(-3, 1, 20)  \n",
    "\n",
    "    best_gamma, best_nmi = find_optimal_gamma(\n",
    "        X=X,\n",
    "        y_true=y_true,\n",
    "        n_clusters=n_clusters,\n",
    "        gamma_range=gamma_candidates\n",
    "    )\n",
    "\n",
    "    print(f\"\\nbest gamma: {best_gamma:.5f}\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    \n",
    "    algorithms = {\n",
    "        \n",
    "        'SCPSO': SCPSO(n_clusters=n_clusters, n_particles=30, max_iter=50, gamma=best_gamma)\n",
    "    }\n",
    "        \n",
    "    for algo_name, algo in algorithms.items():\n",
    "        try:\n",
    "            print(f\"Running {algo_name}...\")\n",
    "            y_pred = algo.fit_predict(X)\n",
    "            results[f\"{algo_name}\"] = evaluate_clustering(y_true, y_pred)\n",
    "            print(f\"{algo_name} completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {algo_name} : {str(e)}\")\n",
    "            results[f\"{algo_name}\"] = {'Purity': 0, 'NMI': 0, 'ARI': 0}\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "results = run_AG_dataset()\n",
    "\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f28aefe-37dc-4dd3-b415-3d4e0425cb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results on BBCSport Dataset ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [01:14<00:00,  3.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best gamma: 0.00100\n",
      "Running SCPSO...\n",
      "SCPSO completed\n",
      "\n",
      "SCPSO:\n",
      "Purity: 0.4559\n",
      "NMI: 0.2120\n",
      "ARI: 0.1327\n"
     ]
    }
   ],
   "source": [
    "#BBC Sport\n",
    "def run_BBCS(name, X, y_true, n_clusters):\n",
    "    print(f\"\\n=== Results on {name} Dataset ===\")\n",
    "    \n",
    "    gamma_candidates = np.logspace(-3, 1, 20)  \n",
    "\n",
    "    best_gamma, best_nmi = find_optimal_gamma(\n",
    "        X=X,\n",
    "        y_true=y_true,\n",
    "        n_clusters=n_clusters,\n",
    "        gamma_range=gamma_candidates\n",
    "    )\n",
    "\n",
    "    print(f\"\\nbest gamma: {best_gamma:.5f}\")\n",
    "    \n",
    "    results = {}\n",
    "    algorithms = {\n",
    "        'SCPSO': SCPSO(n_clusters=n_clusters, n_particles=30, max_iter=50, gamma=best_gamma)\n",
    "            \n",
    "    }\n",
    "        \n",
    "    for algo_name, algo in algorithms.items():\n",
    "        try:\n",
    "            print(f\"Running {algo_name}...\")\n",
    "            y_pred = algo.fit_predict(X)\n",
    "            results[f\"{algo_name}\"] = evaluate_clustering(y_true, y_pred)\n",
    "            print(f\"{algo_name} completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {algo_name} : {str(e)}\")\n",
    "            results[f\"{algo_name}\"] = {'Purity': 0, 'NMI': 0, 'ARI': 0}\n",
    "   \n",
    "    \n",
    "\n",
    "    for method, metrics in results.items():\n",
    "        print(f\"\\n{method}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "\n",
    "bbcsport_X = pd.read_csv(r'C:\\Users\\asus\\Downloads\\FinalProject\\archive (2)\\bbcsport_mtx.csv', header=None).values\n",
    "bbcsport_y = pd.read_csv(r'C:\\Users\\asus\\Downloads\\FinalProject\\archive (2)\\bbcsport_classes.csv', header=None)[0].values\n",
    "bbcsport_n_clusters = len(np.unique(bbcsport_y))\n",
    "\n",
    "run_BBCS(\"BBCSport\", bbcsport_X, bbcsport_y, bbcsport_n_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e104f93c-b09d-4806-8178-7fb847673ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d542d93-948c-4950-b027-a5ae900cf7c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ecff3f3-ebea-417e-b2e4-8e6633733fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SCPSO...\n",
      "SCPSO completed\n",
      "\n",
      "=== Final Results ===\n",
      "\n",
      "SCPSO:\n",
      "Purity: 0.8659\n",
      "NMI: 0.0282\n",
      "ARI: -0.0779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [45:16<00:00, 135.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best gamma: 0.01129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#SMS\n",
    "df = pd.read_csv(\"C:/Users/asus/Downloads/FinalProject/archive (3)/spam.csv\", encoding='latin1')[['v1', 'v2']]\n",
    "df.columns = ['label', 'text']\n",
    "\n",
    "processed_texts = preprocess_text(df['text'].tolist())\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english',\n",
    "             min_df=2,\n",
    "             max_df=0.95,\n",
    "             token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "\n",
    "X = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "label_mapping = {'ham': 0, 'spam': 1}\n",
    "df['label'] = df['label'].map(label_mapping)\n",
    "\n",
    "y_true = df['label'].tolist()\n",
    "\n",
    "n_clusters = len(np.unique(y_true))\n",
    "\n",
    "results = {}\n",
    "\n",
    "algorithms = {\n",
    "    'SCPSO': SCPSO(n_clusters=n_clusters, n_particles=30, max_iter=50, gamma=0.1)\n",
    "            \n",
    "}\n",
    "        \n",
    "for algo_name, algo in algorithms.items():\n",
    "    try:\n",
    "        print(f\"Running {algo_name}...\")\n",
    "        y_pred = algo.fit_predict(X)\n",
    "        results[f\"{algo_name}\"] = evaluate_clustering(y_true, y_pred)\n",
    "        print(f\"{algo_name} completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {algo_name} : {str(e)}\")\n",
    "        results[f\"{algo_name}\"] = {'Purity': 0, 'NMI': 0, 'ARI': 0}\n",
    "    \n",
    "\n",
    "print(\"\\n=== Final Results ===\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "\n",
    "gamma_candidates = np.logspace(-3, 1, 20)  \n",
    "\n",
    "best_gamma, best_nmi = find_optimal_gamma(\n",
    "    X=X,\n",
    "    y_true=y_true,\n",
    "    n_clusters=n_clusters,\n",
    "    gamma_range=gamma_candidates\n",
    ")\n",
    "\n",
    "print(f\"\\nbest gamma: {best_gamma:.5f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "653beae7-4480-41b0-804e-862267ff8e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SCPSO...\n",
      "SCPSO completed\n",
      "\n",
      "=== Final Results ===\n",
      "\n",
      "SCPSO:\n",
      "Purity: 0.8659\n",
      "NMI: 0.0709\n",
      "ARI: -0.0780\n"
     ]
    }
   ],
   "source": [
    "#SMS\n",
    "df = pd.read_csv(\"C:/Users/asus/Downloads/FinalProject/archive (3)/spam.csv\", encoding='latin1')[['v1', 'v2']]\n",
    "df.columns = ['label', 'text']\n",
    "\n",
    "processed_texts = preprocess_text(df['text'].tolist())\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english',\n",
    "             min_df=2,\n",
    "             max_df=0.95,\n",
    "             token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "\n",
    "X = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "label_mapping = {'ham': 0, 'spam': 1}\n",
    "df['label'] = df['label'].map(label_mapping)\n",
    "\n",
    "y_true = df['label'].tolist()\n",
    "\n",
    "n_clusters = len(np.unique(y_true))\n",
    "\n",
    "results = {}\n",
    "\n",
    "algorithms = {\n",
    "    'SCPSO': SCPSO(n_clusters=n_clusters, n_particles=30, max_iter=50, gamma=best_gamma)\n",
    "            \n",
    "}\n",
    "        \n",
    "for algo_name, algo in algorithms.items():\n",
    "    try:\n",
    "        print(f\"Running {algo_name}...\")\n",
    "        y_pred = algo.fit_predict(X)\n",
    "        results[f\"{algo_name}\"] = evaluate_clustering(y_true, y_pred)\n",
    "        print(f\"{algo_name} completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {algo_name} : {str(e)}\")\n",
    "        results[f\"{algo_name}\"] = {'Purity': 0, 'NMI': 0, 'ARI': 0}\n",
    "    \n",
    "\n",
    "print(\"\\n=== Final Results ===\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9f60d656-448c-4d32-90f0-afa66a855f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [02:42<00:00,  8.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best gamma: 0.00162\n",
      "Running SCPSO...\n",
      "SCPSO completed\n",
      "\n",
      "=== Final Results ===\n",
      "\n",
      "SCPSO:\n",
      "Purity: 0.7052\n",
      "NMI: 0.5819\n",
      "ARI: 0.5045\n"
     ]
    }
   ],
   "source": [
    "#BBC news\n",
    "def run_experiments_BBC_dataset():\n",
    "    \n",
    "    df = pd.read_csv(\"C:/Users/asus/Downloads/FinalProject/dataset.csv\", encoding='latin1')\n",
    "\n",
    "    texts = df['news'].astype(str).tolist()\n",
    "    labels = df['type'].astype(str).tolist()\n",
    "    y_true = LabelEncoder().fit_transform(labels)\n",
    "    n_clusters = len(np.unique(y_true))\n",
    "\n",
    "    processed_texts = preprocess_text(texts)\n",
    "\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                 min_df=2,\n",
    "                 max_df=0.95,\n",
    "                 token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "   \n",
    "    \n",
    "    X = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "    gamma_candidates = np.logspace(-3, 1, 20)  \n",
    "    best_gamma, best_nmi = find_optimal_gamma(\n",
    "        X=X,\n",
    "        y_true=y_true,\n",
    "        n_clusters=n_clusters,\n",
    "        gamma_range=gamma_candidates\n",
    "    )\n",
    "\n",
    "    print(f\"\\nbest gamma: {best_gamma:.5f}\")\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    algorithms = {\n",
    "         'SCPSO': SCPSO(n_clusters=n_clusters, n_particles=30, max_iter=50, gamma=best_gamma)\n",
    "            \n",
    "    }\n",
    "        \n",
    "    for algo_name, algo in algorithms.items():\n",
    "        try:\n",
    "            print(f\"Running {algo_name}...\")\n",
    "            y_pred = algo.fit_predict(X)\n",
    "            results[f\"{algo_name}\"] = evaluate_clustering(y_true, y_pred)\n",
    "            print(f\"{algo_name} completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {algo_name} : {str(e)}\")\n",
    "            results[f\"{algo_name}\"] = {'Purity': 0, 'NMI': 0, 'ARI': 0}\n",
    "    \n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "results = run_experiments_BBC_dataset()\n",
    "\n",
    "print(\"\\n=== Final Results ===\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd43e75b-87dc-4b33-b765-4e5cc29b7e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Computer category ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [39:39<00:00, 118.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best gamma: 2.33572\n",
      "Running SCPSO...\n",
      "SCPSO completed for Computer\n",
      "\n",
      "=== Processing Politics category ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [07:47<00:00, 23.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best gamma: 0.00428\n",
      "Running SCPSO...\n",
      "SCPSO completed for Politics\n",
      "\n",
      "=== Processing Miscellaneous category ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [14:55<00:00, 44.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best gamma: 0.54556\n",
      "Running SCPSO...\n",
      "SCPSO completed for Miscellaneous\n",
      "\n",
      "=== Processing Religion category ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [06:17<00:00, 18.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best gamma: 0.02976\n",
      "Running SCPSO...\n",
      "SCPSO completed for Religion\n",
      "\n",
      "=== Processing Science category ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [21:56<00:00, 65.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best gamma: 0.33598\n",
      "Running SCPSO...\n",
      "SCPSO completed for Science\n",
      "\n",
      "=== Final Results ===\n",
      "\n",
      "Computer_SCPSO:\n",
      "Purity: 0.3210\n",
      "NMI: 0.1627\n",
      "ARI: 0.0862\n",
      "\n",
      "Politics_SCPSO:\n",
      "Purity: 0.5337\n",
      "NMI: 0.2059\n",
      "ARI: 0.1049\n",
      "\n",
      "Miscellaneous_SCPSO:\n",
      "Purity: 0.6973\n",
      "NMI: 0.5330\n",
      "ARI: 0.4806\n",
      "\n",
      "Religion_SCPSO:\n",
      "Purity: 0.5219\n",
      "NMI: 0.0993\n",
      "ARI: 0.0920\n",
      "\n",
      "Science_SCPSO:\n",
      "Purity: 0.4226\n",
      "NMI: 0.2515\n",
      "ARI: 0.1063\n"
     ]
    }
   ],
   "source": [
    "#20newsgroup\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "import pyswarms as ps\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_experiments():\n",
    "    categories = {\n",
    "        'Computer': ['comp.graphics', 'comp.os.ms-windows.misc', \n",
    "                    'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', \n",
    "                    'comp.windows.x'],\n",
    "        'Politics': ['talk.politics.misc', 'talk.politics.guns', \n",
    "                    'talk.politics.mideast'],\n",
    "        'Miscellaneous': ['misc.forsale','talk.politics.misc','talk.religion.misc','comp.os.ms-windows.misc'],\n",
    "        'Religion': ['talk.religion.misc', 'alt.atheism', 'soc.religion.christian'],\n",
    "        'Science': ['sci.crypt', 'sci.electronics', 'sci.med', 'sci.space']\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, subcats in categories.items():\n",
    "        print(f\"\\n=== Processing {name} category ===\")\n",
    "        dataset = fetch_20newsgroups(subset='all', categories=subcats, \n",
    "                                   remove=('headers', 'footers', 'quotes'))\n",
    "        n_clusters = len(subcats)\n",
    "        \n",
    "        processed_texts = preprocess_text(dataset.data)\n",
    "\n",
    "\n",
    "        vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                     min_df=2,\n",
    "                     max_df=0.95,\n",
    "                     token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "   \n",
    "    \n",
    "        X = vectorizer.fit_transform(processed_texts)\n",
    "        y_true = dataset.target\n",
    "        \n",
    "        gamma_candidates = np.logspace(-3, 1, 20)  \n",
    "        best_gamma, best_nmi = find_optimal_gamma(\n",
    "            X=X,\n",
    "            y_true=y_true,\n",
    "            n_clusters=n_clusters,\n",
    "            gamma_range=gamma_candidates\n",
    "        )\n",
    "\n",
    "        print(f\"\\nbest gamma: {best_gamma:.5f}\")\n",
    "        \n",
    "        algorithms = {\n",
    "\n",
    "            'SCPSO': SCPSO(n_clusters=n_clusters, n_particles=30, max_iter=50, gamma=best_gamma)\n",
    "               \n",
    "            \n",
    "        }\n",
    "        \n",
    "        for algo_name, algo in algorithms.items():\n",
    "            try:\n",
    "                print(f\"Running {algo_name}...\")\n",
    "                y_pred = algo.fit_predict(X)\n",
    "                results[f\"{name}_{algo_name}\"] = evaluate_clustering(y_true, y_pred)\n",
    "                print(f\"{algo_name} completed for {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {algo_name} for {name}: {str(e)}\")\n",
    "                results[f\"{name}_{algo_name}\"] = {'Purity': 0, 'NMI': 0, 'ARI': 0}\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_experiments()\n",
    "    print(\"\\n=== Final Results ===\")\n",
    "    for name, metrics in results.items():\n",
    "        print(f\"\\n{name}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "811a2ec9-a31e-4863-ab0b-92826946d335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_4664\\3415555063.py:135: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=min(N, len(x)), random_state=42))\n",
      "Finding gamma: 100%|| 20/20 [03:39<00:00, 10.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best gamma: 0.04833\n",
      "Running SCPSO...\n",
      "SCPSO completed\n",
      "\n",
      "=== Final Results ===\n",
      "\n",
      "SCPSO:\n",
      "Purity: 0.1762\n",
      "NMI: 0.0881\n",
      "ARI: 0.0260\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_kernels\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.preprocessing import normalize\n",
    "#DMOZ\n",
    "class SCPSO:\n",
    "    def __init__(self, n_clusters=4, n_particles=20, max_iter=100, w=0.7, c1=1.5, c2=1.5, gamma=0.01, random_state=None):\n",
    "        self.random_state = random_state\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_particles = n_particles\n",
    "        self.max_iter = max_iter\n",
    "        self.w = w\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def _compute_laplacian(self, X):\n",
    "        W = rbf_kernel(X, gamma=self.gamma)\n",
    "        D = np.diag(np.sum(W, axis=1))\n",
    "        D_inv_sqrt = np.linalg.inv(np.sqrt(D))\n",
    "        L = np.eye(len(X)) - D_inv_sqrt @ W @ D_inv_sqrt\n",
    "        return L\n",
    "\n",
    "    def _compute_embedding(self, L):\n",
    "        eigvals, eigvecs = np.linalg.eigh(L)\n",
    "        idx = np.argsort(eigvals)\n",
    "\n",
    "        max_components = eigvecs.shape[1] - 1\n",
    "        n_embedding_components = min(self.n_clusters, max_components)\n",
    "\n",
    "        if n_embedding_components < self.n_clusters:\n",
    "            print(f\"[Warning] Only {n_embedding_components} eigenvectors available for embedding. Clustering will still use {self.n_clusters} clusters.\")\n",
    "\n",
    "        try:\n",
    "            embedding = eigvecs[:, idx[1:self.n_clusters + 1]].real\n",
    "        except IndexError as e:\n",
    "            print(f\"[FATAL] Could not extract embedding: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "\n",
    "        return normalize(embedding)\n",
    "\n",
    "    def _pso_init(self, X_embedding):\n",
    "        n_features = X_embedding.shape[1]\n",
    "        particles = np.random.rand(self.n_particles, self.n_clusters, n_features)\n",
    "        velocities = np.zeros_like(particles)\n",
    "        pbest = particles.copy()\n",
    "        pbest_scores = np.full(self.n_particles, -np.inf)\n",
    "        \n",
    "\n",
    "        return particles, velocities, pbest, pbest_scores\n",
    "\n",
    "    def _compute_fitness(self, X_embedding, centroids):\n",
    "        distances = np.zeros((X_embedding.shape[0], self.n_clusters))\n",
    "        for i in range(self.n_clusters):\n",
    "            distances[:, i] = np.linalg.norm(X_embedding - centroids[i], axis=1)\n",
    "\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "\n",
    "        if len(np.unique(labels)) < self.n_clusters:\n",
    "            return -np.inf\n",
    "\n",
    "        intra_dist = np.mean([np.mean(distances[labels == i, i]) for i in range(self.n_clusters)])\n",
    "        inter_distances = []\n",
    "        for i in range(self.n_clusters):\n",
    "            mask = labels == i\n",
    "            if np.sum(mask) == 0:\n",
    "                return -np.inf\n",
    "            min_dist = np.min([np.mean(distances[mask, j]) for j in range(self.n_clusters) if j != i] or [np.inf])\n",
    "            inter_distances.append(min_dist)\n",
    "\n",
    "        inter_dist = np.mean(inter_distances)\n",
    "        return inter_dist / (intra_dist + 1e-10)\n",
    "\n",
    "    def _pso_optimize(self, X_embedding):\n",
    "        particles, velocities, pbest, pbest_scores = self._pso_init(X_embedding)\n",
    "        gbest = pbest[0].copy()\n",
    "        gbest_score = -np.inf\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            for i in range(self.n_particles):\n",
    "                current_score = self._compute_fitness(X_embedding, particles[i])\n",
    "                if current_score > pbest_scores[i]:\n",
    "                    pbest_scores[i] = current_score\n",
    "                    pbest[i] = particles[i].copy()\n",
    "                if current_score > gbest_score:\n",
    "                    gbest_score = current_score\n",
    "                    gbest = particles[i].copy()\n",
    "\n",
    "            for i in range(self.n_particles):\n",
    "                r1, r2 = np.random.rand(2)\n",
    "                velocities[i] = (self.w * velocities[i] +\n",
    "                                 self.c1 * r1 * (pbest[i] - particles[i]) +\n",
    "                                 self.c2 * r2 * (gbest - particles[i]))\n",
    "                particles[i] += velocities[i]\n",
    "\n",
    "        return gbest\n",
    "\n",
    "    def fit_predict(self, X):\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "        if hasattr(X, 'toarray'):\n",
    "            X = X.toarray()\n",
    "\n",
    "        L = self._compute_laplacian(X)\n",
    "        X_embedding = self._compute_embedding(L)\n",
    "        centroids = self._pso_optimize(X_embedding)\n",
    "\n",
    "        distances = np.zeros((X_embedding.shape[0], self.n_clusters))\n",
    "        for i in range(self.n_clusters):\n",
    "            distances[:, i] = np.linalg.norm(X_embedding - centroids[i], axis=1)\n",
    "\n",
    "        return np.argmin(distances, axis=1)\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/asus/Downloads/FinalProject/archive (5)/dmoz.csv\", encoding='latin1')\n",
    "df = df.rename(columns={\n",
    "            'category': 'Class Index',\n",
    "            'title': 'Title',\n",
    "            'desc': 'Description'\n",
    "        })\n",
    "N = 300  \n",
    "df_balanced = (\n",
    "    df.groupby('Class Index', group_keys=False)\n",
    "    .apply(lambda x: x.sample(n=min(N, len(x)), random_state=42))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n",
    "texts = (df_balanced['Title'] + ' ' + df_balanced['Description']).astype(str).tolist()\n",
    "labels = df_balanced['Class Index'].tolist()\n",
    "\n",
    "\n",
    "processed_texts = preprocess_text(texts)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english',\n",
    "             min_df=2,\n",
    "             max_df=0.95,\n",
    "             token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "\n",
    "X = vectorizer.fit_transform(processed_texts)\n",
    " \n",
    "label_encoder = LabelEncoder()\n",
    "y_true = label_encoder.fit_transform(labels)\n",
    "\n",
    "\n",
    "\n",
    "n_clusters = len(np.unique(y_true))\n",
    "gamma_candidates = np.logspace(-3, 1, 20)  \n",
    "best_gamma, best_nmi = find_optimal_gamma(\n",
    "    X=X,\n",
    "    y_true=y_true,\n",
    "    n_clusters=n_clusters,\n",
    "    gamma_range=gamma_candidates\n",
    ")\n",
    "\n",
    "print(f\"\\nbest gamma: {best_gamma:.5f}\")\n",
    "results = {}\n",
    "\n",
    "algorithms = {\n",
    "    'SCPSO': SCPSO(n_clusters=n_clusters, n_particles=30, max_iter=50, gamma=best_gamma, random_state=42)\n",
    "            \n",
    "}\n",
    "        \n",
    "for algo_name, algo in algorithms.items():\n",
    "    try:\n",
    "        print(f\"Running {algo_name}...\")\n",
    "        y_pred = algo.fit_predict(X)\n",
    "        results[f\"{algo_name}\"] = evaluate_clustering(y_true, y_pred)\n",
    "        print(f\"{algo_name} completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {algo_name} : {str(e)}\")\n",
    "        results[f\"{algo_name}\"] = {'Purity': 0, 'NMI': 0, 'ARI': 0}\n",
    "    \n",
    "\n",
    "print(\"\\n=== Final Results ===\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cfd3ae1c-c898-4757-8b4b-afb638a3720e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Cornell.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [00:15<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma for Cornell.csv: 0.54556\n",
      "Running SCPSO on Cornell.csv...\n",
      "SCPSO completed on Cornell.csv\n",
      "\n",
      "Processing Texas.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [00:14<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma for Texas.csv: 0.54556\n",
      "Running SCPSO on Texas.csv...\n",
      "SCPSO completed on Texas.csv\n",
      "\n",
      "Processing Washington.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [00:16<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma for Washington.csv: 2.33572\n",
      "Running SCPSO on Washington.csv...\n",
      "SCPSO completed on Washington.csv\n",
      "\n",
      "Processing Wisconsin.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [00:16<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma for Wisconsin.csv: 2.33572\n",
      "Running SCPSO on Wisconsin.csv...\n",
      "SCPSO completed on Wisconsin.csv\n",
      "\n",
      "=== Final Results on WebKB ===\n",
      "\n",
      "Cornell:\n",
      "Purity: 0.5183\n",
      "NMI: 0.1719\n",
      "ARI: 0.1029\n",
      "\n",
      "Texas:\n",
      "Purity: 0.5989\n",
      "NMI: 0.2286\n",
      "ARI: 0.0502\n",
      "\n",
      "Washington:\n",
      "Purity: 0.6856\n",
      "NMI: 0.3187\n",
      "ARI: 0.2524\n",
      "\n",
      "Wisconsin:\n",
      "Purity: 0.6642\n",
      "NMI: 0.3279\n",
      "ARI: 0.2362\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def run_experiments_webkb_dataset(webkb_folder_path):\n",
    "    results = {}\n",
    "    gamma_candidates = np.logspace(-3, 1, 20)\n",
    "\n",
    "    for file in os.listdir(webkb_folder_path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(webkb_folder_path, file)\n",
    "            print(f\"\\nProcessing {file}...\")\n",
    "\n",
    "            # Load data\n",
    "            df = pd.read_csv(file_path)\n",
    "            texts = df['raw_text'].astype(str).tolist()\n",
    "            labels = df['label'].tolist()\n",
    "            y_true = LabelEncoder().fit_transform(labels)\n",
    "            n_clusters = len(np.unique(y_true))\n",
    "\n",
    "            # Preprocess\n",
    "            processed_texts = preprocess_text(texts)\n",
    "\n",
    "            # TF-IDF vectorization\n",
    "            vectorizer = TfidfVectorizer(\n",
    "                stop_words='english',\n",
    "                min_df=2,\n",
    "                max_df=0.95,\n",
    "                token_pattern=r'\\b[a-zA-Z]{3,}\\b'\n",
    "            )\n",
    "            X = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "            # Find best gamma for SCPSO\n",
    "            best_gamma, best_nmi = find_optimal_gamma(\n",
    "                X=X,\n",
    "                y_true=y_true,\n",
    "                n_clusters=n_clusters,\n",
    "                gamma_range=gamma_candidates\n",
    "            )\n",
    "            print(f\"Best gamma for {file}: {best_gamma:.5f}\")\n",
    "\n",
    "            # Run SCPSO\n",
    "            try:\n",
    "                algo = SCPSO(n_clusters=n_clusters, n_particles=30, max_iter=50, gamma=best_gamma)\n",
    "                print(f\"Running SCPSO on {file}...\")\n",
    "                y_pred = algo.fit_predict(X)\n",
    "                metrics = evaluate_clustering(y_true, y_pred)\n",
    "                print(f\"SCPSO completed on {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in SCPSO on {file}: {str(e)}\")\n",
    "                metrics = {'Purity': 0, 'NMI': 0, 'ARI': 0}\n",
    "\n",
    "            # Save result\n",
    "            dataset_name = file.replace(\".csv\", \"\")\n",
    "            results[dataset_name] = metrics\n",
    "\n",
    "    return results\n",
    "\n",
    "results = run_experiments_webkb_dataset(\"C:/Users/asus/Downloads/FinalProject/WebKB\")\n",
    "\n",
    "print(\"\\n=== Final Results on WebKB ===\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "27c508b3-88dd-405b-90e9-9fb98f0069a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 1/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_6200\\81062968.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('Class Index', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SCPSO...\n",
      "SCPSO completed\n",
      "\n",
      " Running iteration 2/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_6200\\81062968.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('Class Index', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SCPSO...\n",
      "SCPSO completed\n",
      "\n",
      " Running iteration 3/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_6200\\81062968.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('Class Index', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SCPSO...\n",
      "SCPSO completed\n",
      "\n",
      " Running iteration 4/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_6200\\81062968.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('Class Index', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SCPSO...\n",
      "SCPSO completed\n",
      "\n",
      " Running iteration 5/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_6200\\81062968.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('Class Index', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SCPSO...\n",
      "SCPSO completed\n",
      "\n",
      " Final Aggregated Results over 5 runs:\n",
      "\n",
      " Method: SCPSO\n",
      "Purity: Mean = 0.5258, Std = 0.0636\n",
      "NMI: Mean = 0.2895, Std = 0.0529\n",
      "ARI: Mean = 0.2586, Std = 0.0725\n"
     ]
    }
   ],
   "source": [
    "#standard deviation\n",
    "def run_AG_dataset():\n",
    "    \n",
    "    df = pd.read_csv(\"C:/Users/asus/Downloads/FinalProject/archive (4)/train.csv\", encoding='latin1')\n",
    "    \n",
    "    class_counts = df['Class Index'].value_counts()\n",
    "    min_class_count = class_counts.min()  \n",
    "    samples_per_class = min(295, min_class_count)\n",
    "    \n",
    "    df = df.groupby('Class Index', group_keys=False).apply(\n",
    "        lambda x: x.sample(n=samples_per_class, random_state=42)\n",
    "    )\n",
    "    \n",
    "    texts = (df['Title'] + ' ' + df['Description']).astype(str).tolist()\n",
    "    labels = df['Class Index'].tolist()\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_true = label_encoder.fit_transform(labels)\n",
    "    n_clusters = len(np.unique(y_true))\n",
    "\n",
    "    \n",
    "    processed_texts = preprocess_text(texts)\n",
    "    \n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                 min_df=2,\n",
    "                 max_df=0.95,\n",
    "                 token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "    X = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    \n",
    "    algorithms = {\n",
    "        \n",
    "        'SCPSO': SCPSO(n_clusters=n_clusters, n_particles=30, max_iter=50, gamma=0.33598)\n",
    "    }\n",
    "        \n",
    "    for algo_name, algo in algorithms.items():\n",
    "        try:\n",
    "            print(f\"Running {algo_name}...\")\n",
    "            y_pred = algo.fit_predict(X)\n",
    "            results[f\"{algo_name}\"] = evaluate_clustering(y_true, y_pred)\n",
    "            print(f\"{algo_name} completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {algo_name} : {str(e)}\")\n",
    "            results[f\"{algo_name}\"] = {'Purity': 0, 'NMI': 0, 'ARI': 0}\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "n_runs = 5  \n",
    "all_results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n Running iteration {run + 1}/{n_runs}...\")\n",
    "    run_result = run_AG_dataset()\n",
    "    for method, metrics in run_result.items():\n",
    "        for metric_name, value in metrics.items():\n",
    "            all_results[method][metric_name].append(value)\n",
    "\n",
    "print(\"\\n Final Aggregated Results over\", n_runs, \"runs:\")\n",
    "for method, metric_values in all_results.items():\n",
    "    print(f\"\\n Method: {method}\")\n",
    "    for metric_name, values in metric_values.items():\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        print(f\"{metric_name}: Mean = {mean_val:.4f}, Std = {std_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4390fad1-0465-4bb0-9be1-07630ddbaf64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 1/5...\n",
      "Running SCPSO...\n",
      "SCPSO completed\n",
      "\n",
      " Running iteration 2/5...\n",
      "Running SCPSO...\n",
      "SCPSO completed\n",
      "\n",
      " Running iteration 3/5...\n",
      "Running SCPSO...\n",
      "SCPSO completed\n",
      "\n",
      " Running iteration 4/5...\n",
      "Running SCPSO...\n",
      "SCPSO completed\n",
      "\n",
      " Running iteration 5/5...\n",
      "Running SCPSO...\n",
      "SCPSO completed\n",
      "\n",
      " Final Aggregated Results over 5 runs:\n",
      "\n",
      " Method: SCPSO\n",
      "Purity: Mean = 0.4795, Std = 0.0363\n",
      "NMI: Mean = 0.2144, Std = 0.0381\n",
      "ARI: Mean = 0.1498, Std = 0.0432\n"
     ]
    }
   ],
   "source": [
    "#BBC Sport\n",
    "n_runs = 5\n",
    "aggregated_results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n Running iteration {run+1}/{n_runs}...\")\n",
    "    \n",
    "    results = {}\n",
    "    algorithms = {\n",
    "        'SCPSO': SCPSO(n_clusters=bbcsport_n_clusters, n_particles=30, max_iter=50, gamma=0.001)\n",
    "    }\n",
    "    \n",
    "    for algo_name, algo in algorithms.items():\n",
    "        try:\n",
    "            print(f\"Running {algo_name}...\")\n",
    "            y_pred = algo.fit_predict(bbcsport_X)\n",
    "            results[algo_name] = evaluate_clustering(bbcsport_y, y_pred)\n",
    "            print(f\"{algo_name} completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {algo_name}: {str(e)}\")\n",
    "            results[algo_name] = {'Purity': 0, 'NMI': 0, 'ARI': 0}\n",
    "    \n",
    "    \n",
    "    for method_name, metrics in results.items():\n",
    "        for metric_name, value in metrics.items():\n",
    "            aggregated_results[method_name][metric_name].append(value)\n",
    "\n",
    "\n",
    "print(\"\\n Final Aggregated Results over\", n_runs, \"runs:\")\n",
    "for method_name, metric_values in aggregated_results.items():\n",
    "    print(f\"\\n Method: {method_name}\")\n",
    "    for metric_name, values in metric_values.items():\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        print(f\"{metric_name}: Mean = {mean_val:.4f}, Std = {std_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b0e78677-eac2-4715-a371-bac36601eea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 1/5...\n",
      "Running SCPSO...\n",
      "SCPSO completed\n",
      "\n",
      " Running iteration 2/5...\n",
      "Running SCPSO...\n",
      "SCPSO completed\n",
      "\n",
      " Running iteration 3/5...\n",
      "Running SCPSO...\n",
      "SCPSO completed\n",
      "\n",
      " Running iteration 4/5...\n",
      "Running SCPSO...\n",
      "SCPSO completed\n",
      "\n",
      " Running iteration 5/5...\n",
      "Running SCPSO...\n",
      "SCPSO completed\n",
      "\n",
      " Final Aggregated Results over 5 runs:\n",
      "\n",
      " Method: SCPSO\n",
      "Purity: Mean = 0.8659, Std = 0.0000\n",
      "NMI: Mean = 0.0737, Std = 0.0051\n",
      "ARI: Mean = -0.0748, Std = 0.0054\n"
     ]
    }
   ],
   "source": [
    "#SMS\n",
    "n_runs = 5  \n",
    "aggregated_results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/asus/Downloads/FinalProject/archive (3)/spam.csv\", encoding='latin1')[['v1', 'v2']]\n",
    "df.columns = ['label', 'text']\n",
    "\n",
    "processed_texts = preprocess_text(df['text'].tolist())\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english',\n",
    "             min_df=2,\n",
    "             max_df=0.95,\n",
    "             token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "\n",
    "X = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "label_mapping = {'ham': 0, 'spam': 1}\n",
    "df['label'] = df['label'].map(label_mapping)\n",
    "y_true = df['label'].tolist()\n",
    "n_clusters = len(np.unique(y_true))\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n Running iteration {run+1}/{n_runs}...\")\n",
    "\n",
    "    algorithms = {\n",
    "        'SCPSO': SCPSO(n_clusters=n_clusters, n_particles=30, max_iter=50, gamma=0.01129)\n",
    "    }\n",
    "\n",
    "    for algo_name, algo in algorithms.items():\n",
    "        try:\n",
    "            print(f\"Running {algo_name}...\")\n",
    "            y_pred = algo.fit_predict(X)\n",
    "            metrics = evaluate_clustering(y_true, y_pred)\n",
    "            print(f\"{algo_name} completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {algo_name}: {str(e)}\")\n",
    "            metrics = {'Purity': 0, 'NMI': 0, 'ARI': 0}\n",
    "\n",
    "        for metric_name, value in metrics.items():\n",
    "            aggregated_results[algo_name][metric_name].append(value)\n",
    "\n",
    "print(\"\\n Final Aggregated Results over\", n_runs, \"runs:\")\n",
    "for method_name, metric_values in aggregated_results.items():\n",
    "    print(f\"\\n Method: {method_name}\")\n",
    "    for metric_name, values in metric_values.items():\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        print(f\"{metric_name}: Mean = {mean_val:.4f}, Std = {std_val:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "018129a4-393e-4a1d-bcfb-60d8a704223d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 1/5...\n",
      "Running SCPSO...\n",
      "SCPSO completed\n",
      "\n",
      " Running iteration 2/5...\n",
      "Running SCPSO...\n",
      "SCPSO completed\n",
      "\n",
      " Running iteration 3/5...\n",
      "Running SCPSO...\n",
      "SCPSO completed\n",
      "\n",
      " Running iteration 4/5...\n",
      "Running SCPSO...\n",
      "SCPSO completed\n",
      "\n",
      " Running iteration 5/5...\n",
      "Running SCPSO...\n",
      "SCPSO completed\n",
      "\n",
      " Final Aggregated Results over 5 runs:\n",
      "\n",
      " Method: SCPSO\n",
      "Purity: Mean = 0.6009, Std = 0.0508\n",
      "NMI: Mean = 0.4493, Std = 0.0364\n",
      "ARI: Mean = 0.3379, Std = 0.0673\n"
     ]
    }
   ],
   "source": [
    "#BBC news\n",
    "def run_experiments_BBC_dataset():\n",
    "    \n",
    "    df = pd.read_csv(\"C:/Users/asus/Downloads/FinalProject/dataset.csv\", encoding='latin1')\n",
    "\n",
    "    texts = df['news'].astype(str).tolist()\n",
    "    labels = df['type'].astype(str).tolist()\n",
    "    y_true = LabelEncoder().fit_transform(labels)\n",
    "    n_clusters = len(np.unique(y_true))\n",
    "\n",
    "    processed_texts = preprocess_text(texts)\n",
    "\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                 min_df=2,\n",
    "                 max_df=0.95,\n",
    "                 token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "   \n",
    "    \n",
    "    X = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    algorithms = {\n",
    "         'SCPSO': SCPSO(n_clusters=n_clusters, n_particles=30, max_iter=50, gamma=0.00162)\n",
    "            \n",
    "    }\n",
    "        \n",
    "    for algo_name, algo in algorithms.items():\n",
    "        try:\n",
    "            print(f\"Running {algo_name}...\")\n",
    "            y_pred = algo.fit_predict(X)\n",
    "            results[f\"{algo_name}\"] = evaluate_clustering(y_true, y_pred)\n",
    "            print(f\"{algo_name} completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {algo_name} : {str(e)}\")\n",
    "            results[f\"{algo_name}\"] = {'Purity': 0, 'NMI': 0, 'ARI': 0}\n",
    "    \n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "n_runs = 5  \n",
    "all_results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n Running iteration {run + 1}/{n_runs}...\")\n",
    "    run_result = run_experiments_BBC_dataset()\n",
    "    for method, metrics in run_result.items():\n",
    "        for metric_name, value in metrics.items():\n",
    "            all_results[method][metric_name].append(value)\n",
    "\n",
    "print(\"\\n Final Aggregated Results over\", n_runs, \"runs:\")\n",
    "for method, metric_values in all_results.items():\n",
    "    print(f\"\\n Method: {method}\")\n",
    "    for metric_name, values in metric_values.items():\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        print(f\"{metric_name}: Mean = {mean_val:.4f}, Std = {std_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a930421-d10b-4d7b-ad7f-8dadb2c28747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 1/5...\n",
      "\n",
      "=== Processing Computer category ===\n",
      "Running SCPSO...\n",
      "SCPSO completed for Computer\n",
      "\n",
      " Running iteration 2/5...\n",
      "\n",
      "=== Processing Computer category ===\n",
      "Running SCPSO...\n",
      "SCPSO completed for Computer\n",
      "\n",
      " Running iteration 3/5...\n",
      "\n",
      "=== Processing Computer category ===\n",
      "Running SCPSO...\n",
      "SCPSO completed for Computer\n",
      "\n",
      " Running iteration 4/5...\n",
      "\n",
      "=== Processing Computer category ===\n",
      "Running SCPSO...\n",
      "SCPSO completed for Computer\n",
      "\n",
      " Running iteration 5/5...\n",
      "\n",
      "=== Processing Computer category ===\n",
      "Running SCPSO...\n",
      "SCPSO completed for Computer\n",
      "\n",
      " Final Aggregated Results over 5 runs:\n",
      "\n",
      " Method: Computer_SCPSO\n",
      "Purity: Mean = 0.3369, Std = 0.0144\n",
      "NMI: Mean = 0.1573, Std = 0.0179\n",
      "ARI: Mean = 0.0853, Std = 0.0238\n"
     ]
    }
   ],
   "source": [
    "#20newsgroup\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "import pyswarms as ps\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_experiments():\n",
    "    categories = {\n",
    "        'Computer': ['comp.graphics', 'comp.os.ms-windows.misc', \n",
    "                    'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', \n",
    "                    'comp.windows.x']\n",
    "        \n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, subcats in categories.items():\n",
    "        print(f\"\\n=== Processing {name} category ===\")\n",
    "        dataset = fetch_20newsgroups(subset='all', categories=subcats, \n",
    "                                   remove=('headers', 'footers', 'quotes'))\n",
    "        n_clusters = len(subcats)\n",
    "        \n",
    "        processed_texts = preprocess_text(dataset.data)\n",
    "\n",
    "\n",
    "        vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                     min_df=2,\n",
    "                     max_df=0.95,\n",
    "                     token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "   \n",
    "    \n",
    "        X = vectorizer.fit_transform(processed_texts)\n",
    "        y_true = dataset.target\n",
    "        \n",
    "        \n",
    "        \n",
    "        algorithms = {\n",
    "\n",
    "            'SCPSO': SCPSO(n_clusters=n_clusters, n_particles=30, max_iter=50, gamma=2.33572)\n",
    "               \n",
    "            \n",
    "        }\n",
    "        \n",
    "        for algo_name, algo in algorithms.items():\n",
    "            try:\n",
    "                print(f\"Running {algo_name}...\")\n",
    "                y_pred = algo.fit_predict(X)\n",
    "                results[f\"{name}_{algo_name}\"] = evaluate_clustering(y_true, y_pred)\n",
    "                print(f\"{algo_name} completed for {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {algo_name} for {name}: {str(e)}\")\n",
    "                results[f\"{name}_{algo_name}\"] = {'Purity': 0, 'NMI': 0, 'ARI': 0}\n",
    "    \n",
    "    return results\n",
    "\n",
    "n_runs = 5  \n",
    "all_results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n Running iteration {run + 1}/{n_runs}...\")\n",
    "    run_result = run_experiments()\n",
    "    for method, metrics in run_result.items():\n",
    "        for metric_name, value in metrics.items():\n",
    "            all_results[method][metric_name].append(value)\n",
    "\n",
    "print(\"\\n Final Aggregated Results over\", n_runs, \"runs:\")\n",
    "for method, metric_values in all_results.items():\n",
    "    print(f\"\\n Method: {method}\")\n",
    "    for metric_name, values in metric_values.items():\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        print(f\"{metric_name}: Mean = {mean_val:.4f}, Std = {std_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2cf03a2-f17f-448e-9bf3-123c7c2420bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 1/5...\n",
      "\n",
      "=== Processing Politics category ===\n",
      "Running SCPSO...\n",
      "SCPSO completed for Politics\n",
      "\n",
      " Running iteration 2/5...\n",
      "\n",
      "=== Processing Politics category ===\n",
      "Running SCPSO...\n",
      "SCPSO completed for Politics\n",
      "\n",
      " Running iteration 3/5...\n",
      "\n",
      "=== Processing Politics category ===\n",
      "Running SCPSO...\n",
      "SCPSO completed for Politics\n",
      "\n",
      " Running iteration 4/5...\n",
      "\n",
      "=== Processing Politics category ===\n",
      "Running SCPSO...\n",
      "SCPSO completed for Politics\n",
      "\n",
      " Running iteration 5/5...\n",
      "\n",
      "=== Processing Politics category ===\n",
      "Running SCPSO...\n",
      "SCPSO completed for Politics\n",
      "\n",
      " Final Aggregated Results over 5 runs:\n",
      "\n",
      " Method: Politics_SCPSO\n",
      "Purity: Mean = 0.5078, Std = 0.0265\n",
      "NMI: Mean = 0.1700, Std = 0.0431\n",
      "ARI: Mean = 0.0947, Std = 0.0211\n"
     ]
    }
   ],
   "source": [
    "#20newsgroup\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "import pyswarms as ps\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_experiments():\n",
    "    categories = {\n",
    "        \n",
    "        'Politics': ['talk.politics.misc', 'talk.politics.guns', \n",
    "                    'talk.politics.mideast']\n",
    "        \n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, subcats in categories.items():\n",
    "        print(f\"\\n=== Processing {name} category ===\")\n",
    "        dataset = fetch_20newsgroups(subset='all', categories=subcats, \n",
    "                                   remove=('headers', 'footers', 'quotes'))\n",
    "        n_clusters = len(subcats)\n",
    "        \n",
    "        processed_texts = preprocess_text(dataset.data)\n",
    "\n",
    "\n",
    "        vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                     min_df=2,\n",
    "                     max_df=0.95,\n",
    "                     token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "   \n",
    "    \n",
    "        X = vectorizer.fit_transform(processed_texts)\n",
    "        y_true = dataset.target\n",
    "        \n",
    "        \n",
    "        \n",
    "        algorithms = {\n",
    "\n",
    "            'SCPSO': SCPSO(n_clusters=n_clusters, n_particles=30, max_iter=50, gamma=0.00428)\n",
    "               \n",
    "            \n",
    "        }\n",
    "        \n",
    "        for algo_name, algo in algorithms.items():\n",
    "            try:\n",
    "                print(f\"Running {algo_name}...\")\n",
    "                y_pred = algo.fit_predict(X)\n",
    "                results[f\"{name}_{algo_name}\"] = evaluate_clustering(y_true, y_pred)\n",
    "                print(f\"{algo_name} completed for {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {algo_name} for {name}: {str(e)}\")\n",
    "                results[f\"{name}_{algo_name}\"] = {'Purity': 0, 'NMI': 0, 'ARI': 0}\n",
    "    \n",
    "    return results\n",
    "\n",
    "n_runs = 5  \n",
    "all_results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n Running iteration {run + 1}/{n_runs}...\")\n",
    "    run_result = run_experiments()\n",
    "    for method, metrics in run_result.items():\n",
    "        for metric_name, value in metrics.items():\n",
    "            all_results[method][metric_name].append(value)\n",
    "\n",
    "print(\"\\n Final Aggregated Results over\", n_runs, \"runs:\")\n",
    "for method, metric_values in all_results.items():\n",
    "    print(f\"\\n Method: {method}\")\n",
    "    for metric_name, values in metric_values.items():\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        print(f\"{metric_name}: Mean = {mean_val:.4f}, Std = {std_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a90b2eb7-b057-47ac-9871-5bd25e200a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 1/5...\n",
      "\n",
      "=== Processing Miscellaneous category ===\n",
      "Running SCPSO...\n",
      "SCPSO completed for Miscellaneous\n",
      "\n",
      " Running iteration 2/5...\n",
      "\n",
      "=== Processing Miscellaneous category ===\n",
      "Running SCPSO...\n",
      "SCPSO completed for Miscellaneous\n",
      "\n",
      " Running iteration 3/5...\n",
      "\n",
      "=== Processing Miscellaneous category ===\n",
      "Running SCPSO...\n",
      "SCPSO completed for Miscellaneous\n",
      "\n",
      " Running iteration 4/5...\n",
      "\n",
      "=== Processing Miscellaneous category ===\n",
      "Running SCPSO...\n",
      "SCPSO completed for Miscellaneous\n",
      "\n",
      " Running iteration 5/5...\n",
      "\n",
      "=== Processing Miscellaneous category ===\n",
      "Running SCPSO...\n",
      "SCPSO completed for Miscellaneous\n",
      "\n",
      " Final Aggregated Results over 5 runs:\n",
      "\n",
      " Method: Miscellaneous_SCPSO\n",
      "Purity: Mean = 0.5555, Std = 0.0761\n",
      "NMI: Mean = 0.3742, Std = 0.0778\n",
      "ARI: Mean = 0.3188, Std = 0.1100\n"
     ]
    }
   ],
   "source": [
    "#20newsgroup\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "import pyswarms as ps\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_experiments():\n",
    "    categories = {\n",
    "        'Miscellaneous': ['misc.forsale','talk.politics.misc','talk.religion.misc','comp.os.ms-windows.misc'],\n",
    "        \n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, subcats in categories.items():\n",
    "        print(f\"\\n=== Processing {name} category ===\")\n",
    "        dataset = fetch_20newsgroups(subset='all', categories=subcats, \n",
    "                                   remove=('headers', 'footers', 'quotes'))\n",
    "        n_clusters = len(subcats)\n",
    "        \n",
    "        processed_texts = preprocess_text(dataset.data)\n",
    "\n",
    "\n",
    "        vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                     min_df=2,\n",
    "                     max_df=0.95,\n",
    "                     token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "   \n",
    "    \n",
    "        X = vectorizer.fit_transform(processed_texts)\n",
    "        y_true = dataset.target\n",
    "        \n",
    "\n",
    "        \n",
    "        algorithms = {\n",
    "\n",
    "            'SCPSO': SCPSO(n_clusters=n_clusters, n_particles=30, max_iter=50, gamma=0.54556)\n",
    "               \n",
    "            \n",
    "        }\n",
    "        \n",
    "        for algo_name, algo in algorithms.items():\n",
    "            try:\n",
    "                print(f\"Running {algo_name}...\")\n",
    "                y_pred = algo.fit_predict(X)\n",
    "                results[f\"{name}_{algo_name}\"] = evaluate_clustering(y_true, y_pred)\n",
    "                print(f\"{algo_name} completed for {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {algo_name} for {name}: {str(e)}\")\n",
    "                results[f\"{name}_{algo_name}\"] = {'Purity': 0, 'NMI': 0, 'ARI': 0}\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "n_runs = 5  \n",
    "all_results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n Running iteration {run + 1}/{n_runs}...\")\n",
    "    run_result = run_experiments()\n",
    "    for method, metrics in run_result.items():\n",
    "        for metric_name, value in metrics.items():\n",
    "            all_results[method][metric_name].append(value)\n",
    "\n",
    "print(\"\\n Final Aggregated Results over\", n_runs, \"runs:\")\n",
    "for method, metric_values in all_results.items():\n",
    "    print(f\"\\n Method: {method}\")\n",
    "    for metric_name, values in metric_values.items():\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        print(f\"{metric_name}: Mean = {mean_val:.4f}, Std = {std_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ccf8f456-4a57-4eb0-aa09-1ed30d02b961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 1/5...\n",
      "\n",
      "=== Processing Religion category ===\n",
      "Running SCPSO...\n",
      "SCPSO completed for Religion\n",
      "\n",
      " Running iteration 2/5...\n",
      "\n",
      "=== Processing Religion category ===\n",
      "Running SCPSO...\n",
      "SCPSO completed for Religion\n",
      "\n",
      " Running iteration 3/5...\n",
      "\n",
      "=== Processing Religion category ===\n",
      "Running SCPSO...\n",
      "SCPSO completed for Religion\n",
      "\n",
      " Running iteration 4/5...\n",
      "\n",
      "=== Processing Religion category ===\n",
      "Running SCPSO...\n",
      "SCPSO completed for Religion\n",
      "\n",
      " Running iteration 5/5...\n",
      "\n",
      "=== Processing Religion category ===\n",
      "Running SCPSO...\n",
      "SCPSO completed for Religion\n",
      "\n",
      " Final Aggregated Results over 5 runs:\n",
      "\n",
      " Method: Religion_SCPSO\n",
      "Purity: Mean = 0.4526, Std = 0.0167\n",
      "NMI: Mean = 0.0499, Std = 0.0105\n",
      "ARI: Mean = 0.0273, Std = 0.0130\n"
     ]
    }
   ],
   "source": [
    "#20newsgroup\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "import pyswarms as ps\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_experiments():\n",
    "    categories = {\n",
    "        \n",
    "        'Religion': ['talk.religion.misc', 'alt.atheism', 'soc.religion.christian']\n",
    "        \n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, subcats in categories.items():\n",
    "        print(f\"\\n=== Processing {name} category ===\")\n",
    "        dataset = fetch_20newsgroups(subset='all', categories=subcats, \n",
    "                                   remove=('headers', 'footers', 'quotes'))\n",
    "        n_clusters = len(subcats)\n",
    "        \n",
    "        processed_texts = preprocess_text(dataset.data)\n",
    "\n",
    "\n",
    "        vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                     min_df=2,\n",
    "                     max_df=0.95,\n",
    "                     token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "   \n",
    "    \n",
    "        X = vectorizer.fit_transform(processed_texts)\n",
    "        y_true = dataset.target\n",
    "        \n",
    "       \n",
    "        \n",
    "        algorithms = {\n",
    "\n",
    "            'SCPSO': SCPSO(n_clusters=n_clusters, n_particles=30, max_iter=50, gamma=0.02976)\n",
    "               \n",
    "            \n",
    "        }\n",
    "        \n",
    "        for algo_name, algo in algorithms.items():\n",
    "            try:\n",
    "                print(f\"Running {algo_name}...\")\n",
    "                y_pred = algo.fit_predict(X)\n",
    "                results[f\"{name}_{algo_name}\"] = evaluate_clustering(y_true, y_pred)\n",
    "                print(f\"{algo_name} completed for {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {algo_name} for {name}: {str(e)}\")\n",
    "                results[f\"{name}_{algo_name}\"] = {'Purity': 0, 'NMI': 0, 'ARI': 0}\n",
    "    \n",
    "    return results\n",
    "\n",
    "n_runs = 5  \n",
    "all_results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n Running iteration {run + 1}/{n_runs}...\")\n",
    "    run_result = run_experiments()\n",
    "    for method, metrics in run_result.items():\n",
    "        for metric_name, value in metrics.items():\n",
    "            all_results[method][metric_name].append(value)\n",
    "\n",
    "print(\"\\n Final Aggregated Results over\", n_runs, \"runs:\")\n",
    "for method, metric_values in all_results.items():\n",
    "    print(f\"\\n Method: {method}\")\n",
    "    for metric_name, values in metric_values.items():\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        print(f\"{metric_name}: Mean = {mean_val:.4f}, Std = {std_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05853526-a4f0-45fe-8a16-6bb20c7a7d22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 1/5...\n",
      "\n",
      "=== Processing Science category ===\n",
      "Running SCPSO...\n",
      "SCPSO completed for Science\n",
      "\n",
      " Running iteration 2/5...\n",
      "\n",
      "=== Processing Science category ===\n",
      "Running SCPSO...\n",
      "SCPSO completed for Science\n",
      "\n",
      " Running iteration 3/5...\n",
      "\n",
      "=== Processing Science category ===\n",
      "Running SCPSO...\n",
      "SCPSO completed for Science\n",
      "\n",
      " Running iteration 4/5...\n",
      "\n",
      "=== Processing Science category ===\n",
      "Running SCPSO...\n",
      "SCPSO completed for Science\n",
      "\n",
      " Running iteration 5/5...\n",
      "\n",
      "=== Processing Science category ===\n",
      "Running SCPSO...\n",
      "SCPSO completed for Science\n",
      "\n",
      " Final Aggregated Results over 5 runs:\n",
      "\n",
      " Method: Science_SCPSO\n",
      "Purity: Mean = 0.4865, Std = 0.0692\n",
      "NMI: Mean = 0.2065, Std = 0.0585\n",
      "ARI: Mean = 0.1279, Std = 0.0392\n"
     ]
    }
   ],
   "source": [
    "#20newsgroup\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "import pyswarms as ps\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_experiments():\n",
    "    categories = {\n",
    "       \n",
    "        'Science': ['sci.crypt', 'sci.electronics', 'sci.med', 'sci.space']\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, subcats in categories.items():\n",
    "        print(f\"\\n=== Processing {name} category ===\")\n",
    "        dataset = fetch_20newsgroups(subset='all', categories=subcats, \n",
    "                                   remove=('headers', 'footers', 'quotes'))\n",
    "        n_clusters = len(subcats)\n",
    "        \n",
    "        processed_texts = preprocess_text(dataset.data)\n",
    "\n",
    "\n",
    "        vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                     min_df=2,\n",
    "                     max_df=0.95,\n",
    "                     token_pattern=r'\\b[a-zA-Z]{3,}\\b')\n",
    "   \n",
    "    \n",
    "        X = vectorizer.fit_transform(processed_texts)\n",
    "        y_true = dataset.target\n",
    "        \n",
    "        \n",
    "        \n",
    "        algorithms = {\n",
    "\n",
    "            'SCPSO': SCPSO(n_clusters=n_clusters, n_particles=30, max_iter=50, gamma=0.33598)\n",
    "               \n",
    "            \n",
    "        }\n",
    "        \n",
    "        for algo_name, algo in algorithms.items():\n",
    "            try:\n",
    "                print(f\"Running {algo_name}...\")\n",
    "                y_pred = algo.fit_predict(X)\n",
    "                results[f\"{name}_{algo_name}\"] = evaluate_clustering(y_true, y_pred)\n",
    "                print(f\"{algo_name} completed for {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {algo_name} for {name}: {str(e)}\")\n",
    "                results[f\"{name}_{algo_name}\"] = {'Purity': 0, 'NMI': 0, 'ARI': 0}\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "n_runs = 5  \n",
    "all_results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n Running iteration {run + 1}/{n_runs}...\")\n",
    "    run_result = run_experiments()\n",
    "    for method, metrics in run_result.items():\n",
    "        for metric_name, value in metrics.items():\n",
    "            all_results[method][metric_name].append(value)\n",
    "\n",
    "print(\"\\n Final Aggregated Results over\", n_runs, \"runs:\")\n",
    "for method, metric_values in all_results.items():\n",
    "    print(f\"\\n Method: {method}\")\n",
    "    for metric_name, values in metric_values.items():\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        print(f\"{metric_name}: Mean = {mean_val:.4f}, Std = {std_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3899d9a4-4963-4282-a1c3-c77f8fd79722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running iteration 1...\n",
      "\n",
      "Processing Cornell.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [00:15<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma for Cornell.csv: 0.07848\n",
      "Running SCPSO on Cornell.csv...\n",
      "SCPSO completed on Cornell.csv\n",
      "\n",
      "Processing Texas.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [00:14<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma for Texas.csv: 0.00264\n",
      "Running SCPSO on Texas.csv...\n",
      "SCPSO completed on Texas.csv\n",
      "\n",
      "Processing Washington.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [00:15<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma for Washington.csv: 0.88587\n",
      "Running SCPSO on Washington.csv...\n",
      "SCPSO completed on Washington.csv\n",
      "\n",
      "Processing Wisconsin.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [00:16<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma for Wisconsin.csv: 3.79269\n",
      "Running SCPSO on Wisconsin.csv...\n",
      "SCPSO completed on Wisconsin.csv\n",
      "\n",
      " Running iteration 2...\n",
      "\n",
      "Processing Cornell.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [00:14<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma for Cornell.csv: 3.79269\n",
      "Running SCPSO on Cornell.csv...\n",
      "SCPSO completed on Cornell.csv\n",
      "\n",
      "Processing Texas.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [00:15<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma for Texas.csv: 3.79269\n",
      "Running SCPSO on Texas.csv...\n",
      "SCPSO completed on Texas.csv\n",
      "\n",
      "Processing Washington.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [00:15<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma for Washington.csv: 1.43845\n",
      "Running SCPSO on Washington.csv...\n",
      "SCPSO completed on Washington.csv\n",
      "\n",
      "Processing Wisconsin.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [00:16<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma for Wisconsin.csv: 1.43845\n",
      "Running SCPSO on Wisconsin.csv...\n",
      "SCPSO completed on Wisconsin.csv\n",
      "\n",
      " Running iteration 3...\n",
      "\n",
      "Processing Cornell.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [00:14<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma for Cornell.csv: 0.04833\n",
      "Running SCPSO on Cornell.csv...\n",
      "SCPSO completed on Cornell.csv\n",
      "\n",
      "Processing Texas.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [00:15<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma for Texas.csv: 3.79269\n",
      "Running SCPSO on Texas.csv...\n",
      "SCPSO completed on Texas.csv\n",
      "\n",
      "Processing Washington.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [00:15<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma for Washington.csv: 0.00695\n",
      "Running SCPSO on Washington.csv...\n",
      "SCPSO completed on Washington.csv\n",
      "\n",
      "Processing Wisconsin.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [00:16<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma for Wisconsin.csv: 0.07848\n",
      "Running SCPSO on Wisconsin.csv...\n",
      "SCPSO completed on Wisconsin.csv\n",
      "\n",
      " Running iteration 4...\n",
      "\n",
      "Processing Cornell.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [00:13<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma for Cornell.csv: 2.33572\n",
      "Running SCPSO on Cornell.csv...\n",
      "SCPSO completed on Cornell.csv\n",
      "\n",
      "Processing Texas.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [00:14<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma for Texas.csv: 1.43845\n",
      "Running SCPSO on Texas.csv...\n",
      "SCPSO completed on Texas.csv\n",
      "\n",
      "Processing Washington.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [00:14<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma for Washington.csv: 2.33572\n",
      "Running SCPSO on Washington.csv...\n",
      "SCPSO completed on Washington.csv\n",
      "\n",
      "Processing Wisconsin.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [00:16<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma for Wisconsin.csv: 1.43845\n",
      "Running SCPSO on Wisconsin.csv...\n",
      "SCPSO completed on Wisconsin.csv\n",
      "\n",
      " Running iteration 5...\n",
      "\n",
      "Processing Cornell.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [00:15<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma for Cornell.csv: 0.02976\n",
      "Running SCPSO on Cornell.csv...\n",
      "SCPSO completed on Cornell.csv\n",
      "\n",
      "Processing Texas.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [00:14<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma for Texas.csv: 3.79269\n",
      "Running SCPSO on Texas.csv...\n",
      "SCPSO completed on Texas.csv\n",
      "\n",
      "Processing Washington.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [00:15<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma for Washington.csv: 1.43845\n",
      "Running SCPSO on Washington.csv...\n",
      "SCPSO completed on Washington.csv\n",
      "\n",
      "Processing Wisconsin.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding gamma: 100%|| 20/20 [00:16<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma for Wisconsin.csv: 2.33572\n",
      "Running SCPSO on Wisconsin.csv...\n",
      "SCPSO completed on Wisconsin.csv\n",
      "\n",
      "Final Aggregated Results:\n",
      "\n",
      "=== Cornell ===\n",
      "\n",
      "Method: SCPSO\n",
      "Purity: Mean = 0.5634, Std = 0.0686\n",
      "NMI: Mean = 0.2318, Std = 0.1192\n",
      "ARI: Mean = 0.1300, Std = 0.1176\n",
      "\n",
      "=== Texas ===\n",
      "\n",
      "Method: SCPSO\n",
      "Purity: Mean = 0.6642, Std = 0.0542\n",
      "NMI: Mean = 0.2676, Std = 0.0768\n",
      "ARI: Mean = 0.3682, Std = 0.1088\n",
      "\n",
      "=== Washington ===\n",
      "\n",
      "Method: SCPSO\n",
      "Purity: Mean = 0.6725, Std = 0.0526\n",
      "NMI: Mean = 0.3110, Std = 0.0798\n",
      "ARI: Mean = 0.2840, Std = 0.1012\n",
      "\n",
      "=== Wisconsin ===\n",
      "\n",
      "Method: SCPSO\n",
      "Purity: Mean = 0.6091, Std = 0.0785\n",
      "NMI: Mean = 0.2748, Std = 0.0918\n",
      "ARI: Mean = 0.2464, Std = 0.0903\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def run_experiments_webkb_dataset(webkb_folder_path):\n",
    "    results = {}\n",
    "    gamma_candidates = np.logspace(-3, 1, 20)\n",
    "\n",
    "    for file in os.listdir(webkb_folder_path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(webkb_folder_path, file)\n",
    "            print(f\"\\nProcessing {file}...\")\n",
    "\n",
    "            df = pd.read_csv(file_path)\n",
    "            texts = df['raw_text'].astype(str).tolist()\n",
    "            labels = df['label'].tolist()\n",
    "            y_true = LabelEncoder().fit_transform(labels)\n",
    "            n_clusters = len(np.unique(y_true))\n",
    "\n",
    "            processed_texts = preprocess_text(texts)\n",
    "\n",
    "            vectorizer = TfidfVectorizer(\n",
    "                stop_words='english',\n",
    "                min_df=2,\n",
    "                max_df=0.95,\n",
    "                token_pattern=r'\\b[a-zA-Z]{3,}\\b'\n",
    "            )\n",
    "            X = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "            best_gamma, best_nmi = find_optimal_gamma(\n",
    "                X=X,\n",
    "                y_true=y_true,\n",
    "                n_clusters=n_clusters,\n",
    "                gamma_range=gamma_candidates\n",
    "            )\n",
    "            print(f\"Best gamma for {file}: {best_gamma:.5f}\")\n",
    "\n",
    "            try:\n",
    "                algo = SCPSO(n_clusters=n_clusters, n_particles=30, max_iter=50, gamma=best_gamma)\n",
    "                print(f\"Running SCPSO on {file}...\")\n",
    "                y_pred = algo.fit_predict(X)\n",
    "                metrics = evaluate_clustering(y_true, y_pred)\n",
    "                print(f\"SCPSO completed on {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in SCPSO on {file}: {str(e)}\")\n",
    "                metrics = {'Purity': 0, 'NMI': 0, 'ARI': 0}\n",
    "\n",
    "            dataset_name = file.replace(\".csv\", \"\")\n",
    "            results[dataset_name] = {\"SCPSO\": metrics}  \n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_runs = 5\n",
    "aggregated_results = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n Running iteration {run + 1}...\")\n",
    "    run_results = run_experiments_webkb_dataset(\"C:/Users/asus/Downloads/FinalProject/WebKB\")\n",
    "\n",
    "    \n",
    "    for dataset_name, methods in run_results.items():\n",
    "        for method_name, metrics in methods.items():\n",
    "            for metric_name, value in metrics.items():\n",
    "                aggregated_results[dataset_name][method_name][metric_name].append(value)\n",
    "\n",
    "print(\"\\nFinal Aggregated Results:\")\n",
    "for dataset_name, methods in aggregated_results.items():\n",
    "    print(f\"\\n=== {dataset_name} ===\")\n",
    "    for method_name, metrics in methods.items():\n",
    "        print(f\"\\nMethod: {method_name}\")\n",
    "        for metric_name, values in metrics.items():\n",
    "            mean_val = np.mean(values)\n",
    "            std_val = np.std(values)\n",
    "            print(f\"{metric_name}: Mean = {mean_val:.4f}, Std = {std_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a432b8d2-3fcc-44c5-a4ac-14e86006cd9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55f68a26-133a-4b44-96a9-258b121c1be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Wilcoxon Test for Accuracy ===\n",
      "NMF-FR vs GAKM: p-value = 0.083984 (not significant)\n",
      "NMF-FR vs KM: p-value = 0.001953 (significant)\n",
      "NMF-FR vs SCPSO: p-value = 0.007812 (significant)\n",
      "NMF-FR vs SKM: p-value = 0.130859 (not significant)\n",
      "NMF-FR vs LSAKM: p-value = 0.375000 (not significant)\n",
      "\n",
      "=== Wilcoxon Test for NMI ===\n",
      "NMF-FR vs GAKM: p-value = 0.083984 (not significant)\n",
      "NMF-FR vs KM: p-value = 0.160156 (not significant)\n",
      "NMF-FR vs SCPSO: p-value = 0.037109 (significant)\n",
      "NMF-FR vs SKM: p-value = 0.556641 (not significant)\n",
      "NMF-FR vs LSAKM: p-value = 0.845703 (not significant)\n",
      "\n",
      "=== Wilcoxon Test for ARI ===\n",
      "NMF-FR vs GAKM: p-value = 0.027344 (significant)\n",
      "NMF-FR vs KM: p-value = 0.009766 (significant)\n",
      "NMF-FR vs SCPSO: p-value = 0.009766 (significant)\n",
      "NMF-FR vs SKM: p-value = 0.625000 (not significant)\n",
      "NMF-FR vs LSAKM: p-value = 0.375000 (not significant)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "methods = ['GAKM', 'KM', 'SCPSO', 'SKM', 'NMF-FR', 'LSAKM']\n",
    "\n",
    "\n",
    "accuracy = {\n",
    "    'GAKM':   [0.680, 0.533, 0.582, 0.471, 0.464, 0.441, 0.613, 0.664, 0.210, 0.865],\n",
    "    'KM':     [0.331, 0.464, 0.633, 0.449, 0.482, 0.461, 0.601, 0.898, 0.229, 0.865],\n",
    "    'SCPSO':  [0.321, 0.533, 0.697, 0.521, 0.422, 0.648, 0.455, 0.705, 0.176, 0.865],\n",
    "    'SKM':    [0.395, 0.629, 0.661, 0.471, 0.607, 0.516, 0.802, 0.799, 0.288, 0.881],\n",
    "    'NMF-FR': [0.432, 0.586, 0.777, 0.530, 0.575, 0.639, 0.765, 0.907, 0.376, 0.868],\n",
    "    'LSAKM':  [0.388, 0.587, 0.678, 0.521, 0.529, 0.696, 0.793, 0.933, 0.316, 0.865]\n",
    "}\n",
    "\n",
    "\n",
    "nmi = {\n",
    "    'GAKM':   [0.770, 0.111, 0.230, 0.033, 0.108, 0.100, 0.307, 0.417, 0.093, 0.113],\n",
    "    'KM':     [0.107, 0.120, 0.406, 0.032, 0.405, 0.169, 0.385, 0.761, 0.178, 0.297],\n",
    "    'SCPSO':  [0.162, 0.205, 0.530, 0.099, 0.251, 0.383, 0.212, 0.581, 0.088, 0.070],\n",
    "    'SKM':    [0.163, 0.332, 0.452, 0.057, 0.348, 0.399, 0.771, 0.695, 0.179, 0.300],\n",
    "    'NMF-FR': [0.175, 0.238, 0.531, 0.068, 0.333, 0.361, 0.654, 0.763, 0.231, 0.124],\n",
    "    'LSAKM':  [0.133, 0.288, 0.473, 0.082, 0.220, 0.415, 0.673, 0.815, 0.204, 0.183]\n",
    "}\n",
    "\n",
    "\n",
    "ari = {\n",
    "    'GAKM':   [0.054, 0.115, 0.231, 0.044, 0.101, 0.076, 0.255, 0.410, 0.032, 0.271],\n",
    "    'KM':     [0.042, 0.095, 0.280, 0.047, 0.168, 0.102, 0.243, 0.760, 0.032, 0.050],\n",
    "    'SCPSO':  [0.086, 0.104, 0.480, 0.092, 0.106, 0.404, 0.132, 0.504, 0.026, -0.078],\n",
    "    'SKM':    [0.123, 0.190, 0.425, 0.035, 0.309, 0.379, 0.751, 0.651, 0.082, 0.499],\n",
    "    'NMF-FR': [0.139, 0.254, 0.529, 0.070, 0.299, 0.373, 0.578, 0.784, 0.151, 0.014],\n",
    "    'LSAKM':  [0.091, 0.198, 0.427, 0.063, 0.200, 0.435, 0.582, 0.848, 0.116, 0.018]\n",
    "}\n",
    "\n",
    "\n",
    "def run_wilcoxon(metric_dict, metric_name):\n",
    "    print(f\"\\n=== Wilcoxon Test for {metric_name} ===\")\n",
    "    for method in ['GAKM', 'KM', 'SCPSO', 'SKM', 'LSAKM']:\n",
    "        stat, p = wilcoxon(metric_dict['NMF-FR'], metric_dict[method])\n",
    "        print(f\"NMF-FR vs {method}: p-value = {p:.6f} {'(significant)' if p < 0.05 else '(not significant)'}\")\n",
    "\n",
    "\n",
    "run_wilcoxon(accuracy, \"Accuracy\")\n",
    "run_wilcoxon(nmi, \"NMI\")\n",
    "run_wilcoxon(ari, \"ARI\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2dfa85-f10b-4cae-a150-86a6d83ce723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab19287-1d18-46cc-a2a7-257f7769b546",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
